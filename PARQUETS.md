# ðŸ“Š Parquet Files Documentation

Este documento describe la estructura y el proceso de generaciÃ³n de todos los archivos Parquet producidos por el sistema climaXtreme.

## ðŸ“ Resumen General

El sistema genera **8 archivos Parquet** que contienen diferentes niveles de agregaciÃ³n y anÃ¡lisis de datos climÃ¡ticos histÃ³ricos. Todos estos archivos se generan mediante **Apache Spark** en el contenedor `processor` y se almacenan en **HDFS** bajo el directorio `/data/processed/`.

### Lista de Archivos Generados

| Archivo | DescripciÃ³n | Registros Aprox. | FunciÃ³n Generadora |
|---------|-------------|------------------|-------------------|
| `monthly.parquet` | Agregaciones mensuales por ciudad | ~8.6M | `compute_monthly_aggregations()` |
| `yearly.parquet` | Agregaciones anuales por ciudad | ~350K | `compute_yearly_aggregations()` |
| `anomalies.parquet` | Desviaciones de temperatura respecto a la media climatolÃ³gica | ~350K | `compute_anomalies()` |
| `climatology.parquet` | Valores climatolÃ³gicos promedio por ciudad y mes | ~170K | `compute_climatology()` |
| `seasonal.parquet` | Agregaciones por estaciÃ³n del aÃ±o | ~1M | `compute_seasonal_aggregations()` |
| `extreme_thresholds.parquet` | Umbrales de temperaturas extremas (P10, P90) | ~170K | `compute_extreme_thresholds()` |
| `regional.parquet` | Agregaciones por regiÃ³n geogrÃ¡fica | ~2K | `compute_regional_aggregations()` |
| `continental.parquet` | Agregaciones por continente | ~300 | `compute_continental_aggregations()` |

---

## ðŸ“‹ DescripciÃ³n Detallada de Cada Parquet

### 1ï¸âƒ£ `monthly.parquet`

**PropÃ³sito:** Proporciona agregaciones mensuales de temperatura para cada ciudad, incluyendo estadÃ­sticas descriptivas completas.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del paÃ­s |
| `Latitude` | string | Latitud en formato original (ej: "57.05N") |
| `Longitude` | string | Longitud en formato original (ej: "10.33E") |
| `year` | integer | AÃ±o |
| `month` | integer | Mes (1-12) |
| `avg_temperature` | double | Temperatura promedio mensual (Â°C) |
| `min_temperature` | double | Temperatura mÃ­nima mensual (Â°C) |
| `max_temperature` | double | Temperatura mÃ¡xima mensual (Â°C) |
| `std_temperature` | double | DesviaciÃ³n estÃ¡ndar de la temperatura |
| `record_count` | long | NÃºmero de registros diarios utilizados |

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_monthly_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 309-353)

df_monthly = df.groupBy("City", "Country", "Latitude", "Longitude", "year", "month").agg(
    F.mean("AverageTemperature").alias("avg_temperature"),
    F.min("AverageTemperature").alias("min_temperature"),
    F.max("AverageTemperature").alias("max_temperature"),
    F.stddev("AverageTemperature").alias("std_temperature"),
    F.count("*").alias("record_count")
)
```

**Caso de Uso:** AnÃ¡lisis detallado de tendencias mensuales, series temporales, visualizaciÃ³n de ciclos estacionales por ciudad.

---

### 2ï¸âƒ£ `yearly.parquet`

**PropÃ³sito:** Agrega datos a nivel anual para facilitar anÃ¡lisis de tendencias a largo plazo.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del paÃ­s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `year` | integer | AÃ±o |
| `avg_temperature` | double | Temperatura promedio anual (Â°C) |
| `min_temperature` | double | Temperatura mÃ­nima anual (Â°C) |
| `max_temperature` | double | Temperatura mÃ¡xima anual (Â°C) |
| `temperature_range` | double | Rango de temperatura (max - min) |
| `record_count` | long | NÃºmero de registros mensuales utilizados |

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_yearly_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 355-400)

df_yearly = df_monthly.groupBy("City", "Country", "Latitude", "Longitude", "year").agg(
    F.mean("avg_temperature").alias("avg_temperature"),
    F.min("min_temperature").alias("min_temperature"),
    F.max("max_temperature").alias("max_temperature"),
    (F.max("max_temperature") - F.min("min_temperature")).alias("temperature_range"),
    F.sum("record_count").alias("record_count")
)
```

**Caso de Uso:** VisualizaciÃ³n de tendencias anuales, detecciÃ³n de cambios climÃ¡ticos a largo plazo, comparaciones interanuales.

---

### 3ï¸âƒ£ `anomalies.parquet`

**PropÃ³sito:** Calcula las desviaciones (anomalÃ­as) de temperatura respecto a la media climatolÃ³gica de referencia.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del paÃ­s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `year` | integer | AÃ±o |
| `avg_temperature` | double | Temperatura promedio anual observada (Â°C) |
| `climatology` | double | Temperatura climatolÃ³gica de referencia (Â°C) |
| `anomaly` | double | AnomalÃ­a (observada - climatologÃ­a) (Â°C) |
| `record_count` | long | NÃºmero de registros utilizados |

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_anomalies()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 442-488)

# Se calcula la climatologÃ­a (promedio de todos los aÃ±os)
df_climatology = df_yearly.groupBy("City", "Country", "Latitude", "Longitude").agg(
    F.mean("avg_temperature").alias("climatology")
)

# Se hace JOIN y se calcula la anomalÃ­a
df_anomalies = df_yearly.join(df_climatology, on=["City", "Country", "Latitude", "Longitude"])
df_anomalies = df_anomalies.withColumn("anomaly", 
    F.col("avg_temperature") - F.col("climatology")
)
```

**Caso de Uso:** IdentificaciÃ³n de aÃ±os atÃ­picamente cÃ¡lidos o frÃ­os, anÃ¡lisis de variabilidad climÃ¡tica, estudios de cambio climÃ¡tico.

---

### 4ï¸âƒ£ `climatology.parquet`

**PropÃ³sito:** Proporciona valores climatolÃ³gicos de referencia (promedios histÃ³ricos) por ciudad y mes.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del paÃ­s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `month` | integer | Mes (1-12) |
| `climatology` | double | Temperatura climatolÃ³gica mensual (Â°C) |
| `record_count` | long | NÃºmero de aÃ±os utilizados en el cÃ¡lculo |

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_climatology()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 402-440)

df_climatology = df_monthly.groupBy("City", "Country", "Latitude", "Longitude", "month").agg(
    F.mean("avg_temperature").alias("climatology"),
    F.count("*").alias("record_count")
)
```

**Caso de Uso:** LÃ­nea base para cÃ¡lculo de anomalÃ­as, comparaciÃ³n de condiciones actuales vs. histÃ³ricas, normalizaciÃ³n de datos.

---

### 5ï¸âƒ£ `seasonal.parquet`

**PropÃ³sito:** Agrega datos por estaciones del aÃ±o para analizar patrones estacionales.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del paÃ­s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `year` | integer | AÃ±o |
| `season` | string | EstaciÃ³n: "Winter", "Spring", "Summer", "Fall" |
| `avg_temperature` | double | Temperatura promedio estacional (Â°C) |
| `min_temperature` | double | Temperatura mÃ­nima estacional (Â°C) |
| `max_temperature` | double | Temperatura mÃ¡xima estacional (Â°C) |
| `record_count` | long | NÃºmero de registros mensuales utilizados |

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_seasonal_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 490-526)

# Se asigna la estaciÃ³n segÃºn el mes (hemisferio norte)
df_seasonal = df_monthly.withColumn("season",
    F.when((F.col("month") >= 12) | (F.col("month") <= 2), "Winter")
     .when((F.col("month") >= 3) & (F.col("month") <= 5), "Spring")
     .when((F.col("month") >= 6) & (F.col("month") <= 8), "Summer")
     .otherwise("Fall")
)

# Se agregan por estaciÃ³n
df_seasonal = df_seasonal.groupBy("City", "Country", "Latitude", "Longitude", 
                                   "year", "season").agg(...)
```

**Caso de Uso:** AnÃ¡lisis de variaciones estacionales, identificaciÃ³n de cambios en patrones estacionales, estudios de fenologÃ­a.

---

### 6ï¸âƒ£ `extreme_thresholds.parquet`

**PropÃ³sito:** Calcula umbrales estadÃ­sticos (percentiles 10 y 90) para identificar eventos extremos de temperatura.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del paÃ­s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `month` | integer | Mes (1-12) |
| `p10_temperature` | double | Percentil 10 de temperatura (frÃ­o extremo) (Â°C) |
| `p90_temperature` | double | Percentil 90 de temperatura (calor extremo) (Â°C) |

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_extreme_thresholds()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 672-733)

df_extreme = df_monthly.groupBy("City", "Country", "Latitude", "Longitude", "month").agg(
    F.expr("percentile(avg_temperature, 0.1)").alias("p10_temperature"),
    F.expr("percentile(avg_temperature, 0.9)").alias("p90_temperature")
)
```

**Caso de Uso:** DetecciÃ³n de olas de calor o frÃ­o extremo, alertas tempranas, anÃ¡lisis de eventos climÃ¡ticos extremos.

---

### 7ï¸âƒ£ `regional.parquet`

**PropÃ³sito:** Agrega datos por **16 regiones geogrÃ¡ficas** del mundo, clasificadas automÃ¡ticamente mediante latitud y longitud.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `region` | string | Nombre de la regiÃ³n (ej: "Central Europe", "East Asia") |
| `continent` | string | Continente al que pertenece la regiÃ³n |
| `year` | integer | AÃ±o |
| `avg_temperature` | double | Temperatura promedio regional (Â°C) |
| `min_temperature` | double | Temperatura mÃ­nima regional (Â°C) |
| `max_temperature` | double | Temperatura mÃ¡xima regional (Â°C) |
| `record_count` | long | NÃºmero de ciudades utilizadas |

**Regiones GeogrÃ¡ficas (16):**
1. **Europe:** Northern Europe, Central Europe, Southern Europe
2. **Asia:** Northern Asia, Central Asia, South Asia, East Asia
3. **Africa:** Northern Africa, Central Africa, Southern Africa
4. **North America:** Northern North America, Central North America, Caribbean & Central America
5. **South America:** Northern South America, Central South America, Southern South America
6. **Oceania:** Northern Oceania, Southern Oceania
7. **Antarctica:** Antarctica

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_regional_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 621-670)

# Paso 1: Parsear coordenadas (N/S/E/W â†’ numÃ©rico)
df_with_coords = df_yearly.withColumn("lat_numeric", parse_coordinates_udf(F.col("Latitude"), F.lit("lat")))
df_with_coords = df_with_coords.withColumn("lon_numeric", parse_coordinates_udf(F.col("Longitude"), F.lit("lon")))

# Paso 2: Asignar continente y regiÃ³n
df_with_coords = df_with_coords.withColumn("continent", assign_continent_udf(F.col("lat_numeric"), F.col("lon_numeric")))
df_with_coords = df_with_coords.withColumn("region", assign_region_udf(F.col("lat_numeric"), F.col("lon_numeric")))

# Paso 3: Agregar por regiÃ³n y aÃ±o
df_regional = df_with_coords.groupBy("region", "continent", "year").agg(
    F.mean("avg_temperature").alias("avg_temperature"),
    F.min("min_temperature").alias("min_temperature"),
    F.max("max_temperature").alias("max_temperature"),
    F.sum("record_count").alias("record_count")
)
```

**LÃ³gica de ClasificaciÃ³n Regional:**
- Se parsean las coordenadas de formato "57.05N, 10.33E" a valores numÃ©ricos con signo
- Se aplica una funciÃ³n que clasifica segÃºn rangos de latitud/longitud
- Cada regiÃ³n tiene lÃ­mites geogrÃ¡ficos especÃ­ficos definidos en `assign_region()` (lÃ­neas 583-619)

**Caso de Uso:** ComparaciÃ³n de tendencias climÃ¡ticas entre regiones geogrÃ¡ficas, mapas de calor regionales, dashboard interactivo con selecciÃ³n de regiones.

---

### 8ï¸âƒ£ `continental.parquet`

**PropÃ³sito:** Agrega datos al nivel mÃ¡s alto: **7 continentes**, facilitando comparaciones globales.

**Esquema:**

| Columna | Tipo | DescripciÃ³n |
|---------|------|-------------|
| `continent` | string | Nombre del continente: "Europe", "Asia", "Africa", "North America", "South America", "Oceania", "Antarctica" |
| `year` | integer | AÃ±o |
| `avg_temperature` | double | Temperatura promedio continental (Â°C) |
| `min_temperature` | double | Temperatura mÃ­nima continental (Â°C) |
| `max_temperature` | double | Temperatura mÃ¡xima continental (Â°C) |
| `record_count` | long | NÃºmero de ciudades utilizadas |

**GeneraciÃ³n:**
```python
# FunciÃ³n: compute_continental_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (lÃ­neas 672-733)

# Similar a regional, pero solo agrupa por continente (sin regiÃ³n)
df_continental = df_with_coords.groupBy("continent", "year").agg(
    F.mean("avg_temperature").alias("avg_temperature"),
    F.min("min_temperature").alias("min_temperature"),
    F.max("max_temperature").alias("max_temperature"),
    F.sum("record_count").alias("record_count")
)
```

**LÃ³gica de ClasificaciÃ³n Continental:**
```python
# FunciÃ³n: assign_continent() (lÃ­neas 547-581)
# Clasifica segÃºn rangos de latitud y longitud:
- Europe: lat 36-72, lon -10-40
- Asia: lat -10-81, lon 26-180
- Africa: lat -35-37, lon -18-52
- North America: lat 15-72, lon -169-(-52)
- South America: lat -56-13, lon -82-(-34)
- Oceania: lat -47-(-10), lon 110-180
- Antarctica: lat < -60
```

**Caso de Uso:** Comparaciones globales, visualizaciÃ³n de tendencias continentales, mapas interactivos con burbujas por continente.

---

## ðŸ”„ Proceso de GeneraciÃ³n Completo

### Flujo de Procesamiento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INGESTA (HDFS)                                               â”‚
â”‚    - Archivo: GlobalLandTemperaturesByCity.csv (~500 MB)       â”‚
â”‚    - UbicaciÃ³n: hdfs://namenode:9000/data/raw/                 â”‚
â”‚    - Registros: ~8.6 millones                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. PROCESAMIENTO SPARK (Container: processor)                  â”‚
â”‚    - Script: spark_processor.py                                â”‚
â”‚    - FunciÃ³n principal: process_path()                          â”‚
â”‚    - Orden de ejecuciÃ³n:                                        â”‚
â”‚      a) compute_monthly_aggregations()      â†’ monthly.parquet  â”‚
â”‚      b) compute_yearly_aggregations()       â†’ yearly.parquet   â”‚
â”‚      c) compute_climatology()               â†’ climatology.parq â”‚
â”‚      d) compute_anomalies()                 â†’ anomalies.parq   â”‚
â”‚      e) compute_seasonal_aggregations()     â†’ seasonal.parquet â”‚
â”‚      f) compute_extreme_thresholds()        â†’ extreme_thresh.p â”‚
â”‚      g) compute_regional_aggregations()     â†’ regional.parquet â”‚
â”‚      h) compute_continental_aggregations()  â†’ continental.parq â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ALMACENAMIENTO (HDFS)                                        â”‚
â”‚    - UbicaciÃ³n: hdfs://namenode:9000/data/processed/           â”‚
â”‚    - Formato: Parquet (columnar, comprimido)                   â”‚
â”‚    - TamaÃ±o total estimado: ~100-150 MB                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. VISUALIZACIÃ“N (Dashboard Streamlit)                         â”‚
â”‚    - Acceso directo desde HDFS vÃ­a PyArrow                     â”‚
â”‚    - 6 pestaÃ±as de anÃ¡lisis:                                   â”‚
â”‚      â€¢ Monthly Analysis                                         â”‚
â”‚      â€¢ Yearly Analysis                                          â”‚
â”‚      â€¢ Anomalies Analysis                                       â”‚
â”‚      â€¢ Seasonal Analysis                                        â”‚
â”‚      â€¢ Regional Analysis (con mapa interactivo)                â”‚
â”‚      â€¢ Continental Analysis (con mapa global)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Script de EjecuciÃ³n

**Archivo:** `scripts/process_full_dataset.ps1`

**Comando:**
```powershell
.\scripts\process_full_dataset.ps1
```

**Pasos internos del script:**

1. **Upload a HDFS:**
   ```bash
   docker exec -it namenode hdfs dfs -mkdir -p /data/raw
   docker exec -it namenode hdfs dfs -put /data/GlobalLandTemperaturesByCity.csv /data/raw/
   ```

2. **EjecuciÃ³n Spark:**
   ```bash
   docker exec processor spark-submit \
     --master local[*] \
     /opt/climaxtreme/src/climaxtreme/preprocessing/spark_processor.py \
     --input-path hdfs://namenode:9000/data/raw/GlobalLandTemperaturesByCity.csv \
     --output-path hdfs://namenode:9000/data/processed
   ```

3. **VerificaciÃ³n:**
   ```bash
   docker exec namenode hdfs dfs -ls /data/processed/
   ```

4. **Descarga (Opcional):**
   ```bash
   docker exec namenode hdfs dfs -get /data/processed/*.parquet /data/processed/
   ```

---

## ðŸ› ï¸ Funciones de Procesamiento

### Referencia RÃ¡pida

| FunciÃ³n | LÃ­neas | Dependencias | Output |
|---------|--------|--------------|--------|
| `parse_coordinates()` | 528-545 | - | UDF para parsear "57.05N" â†’ 57.05 |
| `assign_continent()` | 547-581 | parse_coordinates | UDF para asignar continente |
| `assign_region()` | 583-619 | parse_coordinates | UDF para asignar regiÃ³n |
| `compute_monthly_aggregations()` | 309-353 | df (raw) | monthly.parquet |
| `compute_yearly_aggregations()` | 355-400 | monthly.parquet | yearly.parquet |
| `compute_climatology()` | 402-440 | monthly.parquet | climatology.parquet |
| `compute_anomalies()` | 442-488 | yearly + climatology | anomalies.parquet |
| `compute_seasonal_aggregations()` | 490-526 | monthly.parquet | seasonal.parquet |
| `compute_extreme_thresholds()` | 672-733 | monthly.parquet | extreme_thresholds.parquet |
| `compute_regional_aggregations()` | 621-670 | yearly.parquet | regional.parquet |
| `compute_continental_aggregations()` | 672-733 | yearly.parquet (con coords) | continental.parquet |
| `process_path()` | 735-903 | Todas las anteriores | Orquesta todo el proceso |

### Dependencias entre Archivos

```
raw CSV
   â”œâ”€â†’ monthly.parquet (base)
   â”‚      â”œâ”€â†’ yearly.parquet
   â”‚      â”‚      â”œâ”€â†’ anomalies.parquet (yearly + climatology)
   â”‚      â”‚      â”œâ”€â†’ regional.parquet (yearly + coords)
   â”‚      â”‚      â””â”€â†’ continental.parquet (yearly + coords)
   â”‚      â”œâ”€â†’ climatology.parquet
   â”‚      â”œâ”€â†’ seasonal.parquet
   â”‚      â””â”€â†’ extreme_thresholds.parquet
```

---

## ðŸ“ˆ Dashboard Integration

Los archivos Parquet se visualizan en el dashboard Streamlit mediante acceso directo a HDFS:

**Clase:** `HDFSReader` (app.py, lÃ­neas 50-135)

**Ejemplo de lectura:**
```python
# Modo HDFS
reader = HDFSReader(namenode_host="namenode", namenode_port=9000)
df = reader.read_parquet("hdfs://namenode:9000/data/processed/regional.parquet")

# Modo Local
df = pd.read_parquet("./DATA/processed/regional.parquet")
```

**PestaÃ±as del Dashboard:**

1. **Monthly Analysis** â†’ `monthly.parquet`
2. **Yearly Analysis** â†’ `yearly.parquet`
3. **Anomalies Analysis** â†’ `anomalies.parquet`
4. **Seasonal Analysis** â†’ `seasonal.parquet`
5. **Regional Analysis** â†’ `regional.parquet` + **Mapa interactivo del mundo**
6. **Continental Analysis** â†’ `continental.parquet` + **Mapa global con burbujas**

### Visualizaciones de Mapas (Nuevas)

**Regional Analysis Tab:**
- **Mapa Scatter Geo** con 16 regiones
- Burbujas proporcionales a temperatura
- Escala de colores: RdYlBu_r (azul=frÃ­o, rojo=caliente)
- ProyecciÃ³n: Natural Earth
- Tooltip: RegiÃ³n, Temperatura, # Registros

**Continental Analysis Tab:**
- **Mapa Scatter Geo** con 7 continentes
- Burbujas mÃ¡s grandes con texto del continente
- Misma escala de colores
- Tooltip: Continente, Temperatura, # Registros

**Biblioteca Utilizada:** Plotly Express con soporte de `go.Scattergeo`

---

## âœ… ValidaciÃ³n de Datos

### Checks de Calidad

Cada funciÃ³n de procesamiento incluye:
- **Filtrado de valores nulos:** `.filter(F.col("column").isNotNull())`
- **ValidaciÃ³n de rango:** Temperaturas entre -90Â°C y 60Â°C
- **Conteo de registros:** Columna `record_count` para auditorÃ­a

### Logs de Procesamiento

```python
logger.info(f"Monthly aggregations: {df_monthly.count()} records")
logger.info(f"Regional aggregations: {df_regional.count()} records")
logger.info(f"Continental aggregations: {df_continental.count()} records")
```

---

## ðŸ”§ ConfiguraciÃ³n

**Archivo:** `configs/default_config.yml`

```yaml
spark:
  app_name: "ClimaXtreme-Processor"
  master: "local[*]"
  input_path: "hdfs://namenode:9000/data/raw/GlobalLandTemperaturesByCity.csv"
  output_path: "hdfs://namenode:9000/data/processed"
```

---

## ðŸ“š Referencias

- **CÃ³digo fuente:** `Tools/src/climaxtreme/preprocessing/spark_processor.py`
- **Dashboard:** `Tools/src/climaxtreme/dashboard/app.py`
- **Script de procesamiento:** `scripts/process_full_dataset.ps1`
- **ConfiguraciÃ³n:** `Tools/configs/default_config.yml`

---

## ðŸš€ PrÃ³ximos Pasos

- [ ] Agregar tests unitarios para funciones de clasificaciÃ³n geogrÃ¡fica
- [ ] Implementar particionamiento de Parquet por aÃ±o para optimizar lecturas
- [ ] Crear Ã­ndices espaciales para queries geogrÃ¡ficas mÃ¡s rÃ¡pidas
- [ ] AÃ±adir soporte para datos en tiempo real (streaming)

---

**Ãšltima actualizaciÃ³n:** 2024
**Autor:** Equipo climaXtreme
