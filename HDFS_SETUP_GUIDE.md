# Gu√≠a de Configuraci√≥n HDFS para climaXtreme

> **Nota**: Esta gu√≠a soporta **Windows (PowerShell)** y **Linux/macOS (Bash)**. Los comandos espec√≠ficos est√°n claramente marcados.

## ÔøΩ Selecciona tu Sistema Operativo

Esta gu√≠a proporciona comandos para ambas plataformas:

| Sistema Operativo | Shell | Scripts ubicados en |
|-------------------|-------|---------------------|
| **Windows** | PowerShell | `scripts/windows/` |
| **Linux/macOS** | Bash | `scripts/linux/` |

### Diferencias Clave

| Caracter√≠stica | Windows | Linux/macOS |
|----------------|---------|-------------|
| **Scripts** | `.ps1` (PowerShell) | `.sh` (Bash) |
| **Paths** | `\` (backslash) | `/` (forward slash) |
| **Flags** | `-ParameterName` | `--parameter-name` |
| **Ejecuci√≥n** | `.\script.ps1` | `bash script.sh` o `./script.sh` |

### Configuraci√≥n Inicial

**Windows**: No requiere configuraci√≥n adicional (PowerShell est√° incluido)

**Linux/macOS**: Dar permisos de ejecuci√≥n a los scripts (solo primera vez)
```bash
chmod +x scripts/linux/*.sh
```

---

## ÔøΩüöÄ Quick Start (2 comandos)

### Windows (PowerShell)
```powershell
# 1. Levantar todos los contenedores (HDFS + Dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (autom√°tico)
cd ..
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# 3. Abrir dashboard: http://localhost:8501
```

### Linux/macOS (Bash)
```bash
# 1. Levantar todos los contenedores (HDFS + Dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (autom√°tico)
cd ..
bash scripts/linux/process_full_dataset.sh --skip-download

# 3. Abrir dashboard: http://localhost:8501
```

**Resultado**: Abre http://localhost:8501 ‚Üí Selecciona "HDFS" en el sidebar ‚Üí ¬°Listo! üéâ

**Tiempo total: ~30-40 minutos** (la mayor parte es procesamiento autom√°tico en Docker)

**Nota:** Todo se ejecuta en **Docker** (HDFS + Processor + Dashboard). El dashboard ya est√° levantado con `docker-compose up -d`.

---

## Requisitos Previos

1. **Docker Desktop** instalado y en ejecuci√≥n (WSL2 recomendado)
2. **Dataset** en `DATA/GlobalLandTemperaturesByCity.csv`

**NOTA**: Todo el procesamiento se realiza dentro de contenedores Docker. No necesitas instalar Python, PySpark ni Java en tu m√°quina local.

## Paso 1: Verificar Docker Desktop

Antes de ejecutar cualquier comando, aseg√∫rate que Docker Desktop est√° corriendo:

```powershell
docker info
```

Si ves un error, abre Docker Desktop y espera a que muestre "Running" en verde.

## Paso 2: Iniciar Contenedores HDFS y Processor

Desde el directorio `infra` en PowerShell:

```powershell
cd infra
docker-compose up -d
```

Esto iniciar√° **3 contenedores**:
- **climaxtreme-namenode**: HDFS NameNode (puerto 9870 para UI, 9000 para datos)
- **climaxtreme-datanode**: HDFS DataNode
- **climaxtreme-processor**: Contenedor con Python 3.9, PySpark y Java 17 para procesamiento

**Tiempo esperado**: 
- Primera vez: 5-10 minutos (descarga de im√°genes Docker + build del processor)
- Siguientes veces: 10-30 segundos

### Verificar que los contenedores est√°n corriendo:

```powershell
docker-compose ps
```

Deber√≠as ver los 3 contenedores en estado "Running" o "Up", y namenode/datanode con "(healthy)".

## Paso 3: Procesamiento Completo Autom√°tico (Recomendado)

**Usa el script unificado que hace TODO autom√°ticamente:**

### Windows (PowerShell)
```powershell
# Desde el directorio ra√≠z del proyecto
.\scripts\windows\process_full_dataset.ps1
```

### Linux/macOS (Bash)
```bash
# Desde el directorio ra√≠z del proyecto
bash scripts/linux/process_full_dataset.sh
```

Este script ejecuta el **pipeline completo**:
1. ‚úÖ Verifica que Docker est√© corriendo
2. ‚úÖ Inicia contenedores HDFS (namenode + datanode + processor)
3. ‚úÖ Sube el dataset completo a HDFS
4. ‚úÖ Ejecuta procesamiento Spark (genera 11 archivos Parquet)
5. ‚úÖ (Opcional) Descarga resultados a `DATA/processed/`

**Par√°metros disponibles:**

### Windows (PowerShell)
```powershell
# Procesar todo (default: sube, procesa, y descarga)
.\scripts\windows\process_full_dataset.ps1

# Procesar sin descargar (datos quedan solo en HDFS - RECOMENDADO)
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# Solo procesar (asume que datos ya est√°n en HDFS)
.\scripts\windows\process_full_dataset.ps1 -SkipUpload

# Solo procesar (sin upload ni download)
.\scripts\windows\process_full_dataset.ps1 -SkipUpload -SkipDownload

# Especificar ruta del CSV
.\scripts\windows\process_full_dataset.ps1 -CsvPath "DATA\MiArchivo.csv"
```

### Linux/macOS (Bash)
```bash
# Procesar todo (default: sube, procesa, y descarga)
bash scripts/linux/process_full_dataset.sh

# Procesar sin descargar (datos quedan solo en HDFS - RECOMENDADO)
bash scripts/linux/process_full_dataset.sh --skip-download

# Solo procesar (asume que datos ya est√°n en HDFS)
bash scripts/linux/process_full_dataset.sh --skip-upload

# Solo procesar (sin upload ni download)
bash scripts/linux/process_full_dataset.sh --skip-upload --skip-download

# Especificar ruta del CSV
bash scripts/linux/process_full_dataset.sh --csv-path "DATA/MiArchivo.csv"
```

**Salida esperada:**
```
========================================
  climaXtreme - Procesamiento Completo
========================================

PASO 1/4: Cargando dataset COMPLETO a HDFS...
‚úì Dataset cargado exitosamente a HDFS

PASO 2/4: Ejecutando procesamiento con PySpark...
‚úì Procesamiento completado (11 archivos generados)

PASO 3/4: Verificando archivos en HDFS...
‚úì Todos los archivos Parquet verificados

PASO 4/4: Descargando resultados...
  ‚úì monthly.parquet descargado
  ‚úì yearly.parquet descargado
  ... (11 archivos total)

=========================================
  ‚úì PROCESAMIENTO COMPLETADO
=========================================
```

**Archivos generados en HDFS:**
- `/data/climaxtreme/processed/monthly.parquet`
- `/data/climaxtreme/processed/yearly.parquet`
- `/data/climaxtreme/processed/anomalies.parquet`
- `/data/climaxtreme/processed/climatology.parquet`
- `/data/climaxtreme/processed/seasonal.parquet`
- `/data/climaxtreme/processed/extreme_thresholds.parquet`
- `/data/climaxtreme/processed/regional.parquet`
- `/data/climaxtreme/processed/continental.parquet`
- `/data/climaxtreme/processed/correlation_matrix.parquet`
- `/data/climaxtreme/processed/descriptive_stats.parquet`
- `/data/climaxtreme/processed/chi_square_tests.parquet`

---

## Paso 3 Alternativo: Carga Manual (Solo para desarrollo)

Si necesitas cargar datos manualmente sin procesamiento:

### Windows (PowerShell)
```powershell
# Cargar solo una muestra (100,000 filas)
.\scripts\windows\hdfs_setup_and_load.ps1 -CsvPath "DATA\GlobalLandTemperaturesByCity.csv" -Head 100000

# Cargar archivo completo
.\scripts\windows\hdfs_setup_and_load.ps1 -CsvPath "DATA\GlobalLandTemperaturesByCity.csv" -FullFile
```

**Par√°metros del script manual (Windows)**:
- `-CsvPath`: Ruta al archivo CSV original
- `-Head`: N√∫mero de filas a cargar (para muestras)
- `-FullFile`: Cargar archivo completo (sin l√≠mite de filas)

### Linux/macOS (Bash)
```bash
# Cargar solo una muestra (100,000 filas)
bash scripts/linux/hdfs_setup_and_load.sh --csv-path "DATA/GlobalLandTemperaturesByCity.csv" --head 100000

# Cargar archivo completo
bash scripts/linux/hdfs_setup_and_load.sh --csv-path "DATA/GlobalLandTemperaturesByCity.csv" --full-file
```

**Par√°metros del script manual (Linux/macOS)**:
- `--csv-path`: Ruta al archivo CSV original
- `--head`: N√∫mero de filas a cargar (para muestras)
- `--full-file`: Cargar archivo completo (sin l√≠mite de filas)

## Paso 4: Verificar HDFS

### Opci√≥n A: UI Web
Abre en tu navegador: http://localhost:9870

Ve a "Utilities" ‚Üí "Browse the file system" ‚Üí Navega a `/data/climaxtreme`

### Opci√≥n B: L√≠nea de comandos
```powershell
# Ver archivos raw (CSV original)
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme

# Ver archivos procesados (Parquet)
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# Ver detalles de un archivo espec√≠fico
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed/monthly.parquet

# Ver primeras l√≠neas del CSV
docker exec climaxtreme-namenode hdfs dfs -tail /data/climaxtreme/GlobalLandTemperaturesByCity.csv
```

---

## Paso 5: Lanzar Dashboard de Streamlit

**NOTA**: Para usar el dashboard necesitas instalar el paquete climaxtreme en tu m√°quina local.

### Instalaci√≥n local (solo para el dashboard):

```powershell
# Desde el directorio Tools
cd Tools
pip install -e .
```

### Ejecutar dashboard:

```powershell
# El dashboard se ejecuta en DOCKER (contenedor dedicado)
# Ya est√° levantado si hiciste: docker-compose up -d

# Para iniciarlo por separado:
cd infra
docker-compose up -d dashboard

# Para ver logs del dashboard:
docker logs -f climaxtreme-dashboard
```

Abre: http://localhost:8501

### Configurar fuente de datos:

En el **sidebar del dashboard** ver√°s un selector de fuente de datos:

**Opci√≥n 1 - HDFS (Recomendado para Big Data):**
1. Seleccionar: **HDFS (Recommended)**
2. Configurar:
   - HDFS Host: `namenode`
   - HDFS Port: `9000`
   - HDFS Base Path: `/data/climaxtreme/processed`
3. El dashboard leer√° directo desde HDFS sin descargar archivos

**Archivos disponibles para visualizar:**
- **monthly.parquet**: An√°lisis de tendencias mensuales
- **yearly.parquet**: An√°lisis de tendencias anuales con l√≠nea de tendencia
- **anomalies.parquet**: Detecci√≥n de anomal√≠as clim√°ticas
- **seasonal.parquet**: Patrones estacionales
- **extreme_thresholds.parquet**: Umbrales de eventos extremos
- **regional.parquet**: An√°lisis por regi√≥n geogr√°fica (16 regiones) + mapa interactivo üó∫Ô∏è
- **continental.parquet**: An√°lisis por continente (7 continentes) + mapa global üåç
- **correlation_matrix.parquet**: Matriz de correlaci√≥n de Pearson (EDA) üìä
- **descriptive_stats.parquet**: Estad√≠sticas descriptivas completas (EDA) üìà
- **chi_square_tests.parquet**: Pruebas de independencia Chi-cuadrado (EDA) üß™

**Pesta√±as del dashboard (7 en total):**
1. üå°Ô∏è **Temperature Trends**: Tendencias de temperatura a lo largo del tiempo
2. üó∫Ô∏è **Heatmaps**: Mapas de calor de temperatura
3. üìà **Seasonal Analysis**: An√°lisis estacional
4. ‚ö° **Extreme Events**: Eventos clim√°ticos extremos
5. üåç **Regional Analysis**: An√°lisis por regi√≥n + mapa mundial interactivo
6. üåê **Continental Analysis**: An√°lisis por continente + mapa global con burbujas
7. üìä **Exploratory Analysis (EDA)**: Correlaciones, estad√≠sticas descriptivas y pruebas Chi-cuadrado

**Opci√≥n 2 - Local Files (Para desarrollo/demos):**
1. Seleccionar: **Local Files**
2. Primero descarga los archivos desde HDFS (ver secci√≥n de comandos √∫tiles)
3. El dashboard leer√° desde `DATA/processed/`

**Ventajas del modo HDFS:**
- ‚úÖ Sin descargas innecesarias
- ‚úÖ HDFS como √∫nica fuente de verdad (principio Big Data)
- ‚úÖ Siempre datos actualizados
- ‚úÖ Ahorro de espacio en disco local
- ‚úÖ Acceso a todos los 11 archivos Parquet generados
- ‚úÖ Visualizaciones interactivas (mapas, heatmaps, estad√≠sticas)

## Arquitectura del Sistema

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Docker Network (hdfs)                 ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ  NameNode    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  DataNode    ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  (Port 9870) ‚îÇ      ‚îÇ              ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  (Port 9000) ‚îÇ      ‚îÇ              ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ         ‚ñ≤                                                ‚îÇ
‚îÇ         ‚îÇ                                                ‚îÇ
‚îÇ         ‚îÇ HDFS Protocol                                 ‚îÇ
‚îÇ         ‚îÇ                                                ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                ‚îÇ
‚îÇ  ‚îÇ       Processor Container           ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - Python 3.9                       ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - PySpark 3.4+                     ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - Java 17 (OpenJDK)                ‚îÇ                ‚îÇ
‚îÇ  ‚îÇ  - climaxtreme package              ‚îÇ                ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñ≤
         ‚îÇ docker exec
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Windows  ‚îÇ
    ‚îÇ PowerShell‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Soluci√≥n de Problemas

### Error: "image not found" o build muy lento
**Problema**: Docker no puede descargar las im√°genes o el build del processor es lento.

**Soluci√≥n**:
1. Verifica tu conexi√≥n a internet
2. La primera vez, el build del processor descarga muchas dependencias Python (~500MB)
3. Si est√°s detr√°s de un proxy corporativo, config√∫ralo en Docker Desktop ‚Üí Settings ‚Üí Resources ‚Üí Proxies

### Error: "Unable to load native-hadoop library"
**Problema**: Advertencia al ejecutar PySpark (no es cr√≠tico).

**Soluci√≥n**: Es solo una advertencia, el procesamiento funciona correctamente. Hadoop usa librer√≠as Java en su lugar.

### Error: "TypeError: bad operand type for abs(): 'Column'"
**Problema**: Versi√≥n antigua del c√≥digo en el contenedor.

**Soluci√≥n**: Reconstruir la imagen del processor:
```powershell
cd infra
docker-compose build processor
docker-compose stop processor
docker-compose up -d processor
```

### Error: "Connection refused" al conectar con HDFS
**Problema**: Intentas ejecutar `climaxtreme` desde tu m√°quina local en lugar del contenedor.

**Soluci√≥n**: **SIEMPRE** ejecuta los comandos de procesamiento dentro del contenedor:
```powershell
docker exec climaxtreme-processor climaxtreme preprocess ...
```

NO ejecutes directamente:
```powershell
# ‚ùå INCORRECTO - No funcionar√° desde Windows
climaxtreme preprocess --input-path "hdfs://..."

# ‚úÖ CORRECTO - Ejecutar dentro del contenedor
docker exec climaxtreme-processor climaxtreme preprocess --input-path "hdfs://..."
```

### Error: "The system cannot find the file specified"
**Problema**: Docker Desktop no est√° corriendo.

**Soluci√≥n**: Abre Docker Desktop y espera a que est√© en estado "Running"

### Error: "No se pudo iniciar el contenedor" o contenedores no saludables
**Problema**: Los contenedores no arrancan correctamente.

**Soluci√≥n**:
1. Ver logs de los contenedores:
   ```powershell
   docker logs climaxtreme-namenode
   docker logs climaxtreme-datanode
   docker logs climaxtreme-processor
   ```
2. Verificar el estado:
   ```powershell
   cd infra
   docker-compose ps
   ```
3. Reiniciar desde cero:
   ```powershell
   docker-compose down -v
   docker-compose up -d
   ```

### Los contenedores se detienen despu√©s de unos segundos
**Problema**: El processor est√° configurado para mantenerse corriendo indefinidamente.

**Soluci√≥n**: Si el processor se detiene, verifica los logs:
```powershell
docker logs climaxtreme-processor
```

El contenedor debe ejecutar `tail -f /dev/null` para mantenerse activo.

### El dashboard no carga los datos
**Problema**: No se ven archivos disponibles en el dashboard.

**Soluci√≥n**: 
- **Modo HDFS**: Verifica que los contenedores est√©n corriendo y que los archivos existan en HDFS:
  ```powershell
  docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed
  ```
- **Modo Local Files**: Verifica que `DATA/processed/` contenga archivos `.parquet`. Si no existen, desc√°rgalos desde HDFS:
  ```powershell
  docker exec climaxtreme-namenode hdfs dfs -get /data/climaxtreme/processed/*.parquet /tmp/
  docker cp climaxtreme-namenode:/tmp/monthly.parquet ./DATA/processed/
  ```

### Error: "Could not connect to HDFS" en el dashboard
**Problema**: El dashboard no puede conectarse al namenode.

**Soluci√≥n**:
1. Verifica que los contenedores est√©n corriendo:
   ```powershell
   docker ps | Select-String "namenode"
   ```
2. Verifica la configuraci√≥n en el sidebar:
   - HDFS Host debe ser: `namenode` (o `localhost` si est√°s en Windows)
   - HDFS Port debe ser: `9000`
3. Si usas `localhost` como host, puede que necesites usar la IP del contenedor:
   ```powershell
   docker inspect climaxtreme-namenode -f '{{.NetworkSettings.Networks.hdfs.IPAddress}}'
   ```

## Detener HDFS

```powershell
cd infra
docker-compose down
```

Para eliminar tambi√©n los vol√∫menes (datos persistentes):
```powershell
docker-compose down -v
```

**NOTA**: Si ejecutas `down -v`, perder√°s todos los datos en HDFS y tendr√°s que volver a cargarlos.

## Comandos √ötiles

### Docker & Contenedores

```powershell
# Ver todos los contenedores corriendo
docker ps

# Ver logs en tiempo real
docker logs -f climaxtreme-namenode
docker logs -f climaxtreme-processor

# Entrar al contenedor (bash interactivo)
docker exec -it climaxtreme-namenode bash
docker exec -it climaxtreme-processor bash

# Reiniciar un contenedor espec√≠fico
cd infra
docker-compose restart processor
```

### HDFS (ejecutar desde cualquier lugar)

```powershell
# Listar archivos en HDFS
docker exec climaxtreme-namenode hdfs dfs -ls /
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme

# Ver las √∫ltimas l√≠neas de un archivo
docker exec climaxtreme-namenode hdfs dfs -tail /data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv

# Ver espacio usado en HDFS
docker exec climaxtreme-namenode hdfs dfs -df -h

# Descargar archivo desde HDFS a tu m√°quina (solo si usas modo Local Files)
docker exec climaxtreme-namenode hdfs dfs -get /data/climaxtreme/processed/monthly.parquet /tmp/
docker cp climaxtreme-namenode:/tmp/monthly.parquet ./

# NOTA: Con modo HDFS en el dashboard, NO necesitas descargar archivos

# Eliminar archivo/directorio en HDFS
docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme/processed
```

### Procesamiento (ejecutar desde cualquier lugar)

```powershell
# Ejecutar preprocesamiento
docker exec climaxtreme-processor climaxtreme preprocess `
  --input-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv" `
  --output-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed" `
  --format city-csv

# Ver la ayuda del CLI
docker exec climaxtreme-processor climaxtreme --help
docker exec climaxtreme-processor climaxtreme preprocess --help

# Ejecutar Python interactivo dentro del processor
docker exec -it climaxtreme-processor python3
```

### Reconstruir el Processor (cuando cambias c√≥digo)

```powershell
cd infra
docker-compose build processor
docker-compose stop processor
docker-compose up -d processor
```

## Pr√≥ximos Pasos

1. **A√±adir m√°s datos**: Modifica el par√°metro `-Head` del script para cargar m√°s filas, o elim√≠nalo para cargar todo el dataset
2. **Dashboard con HDFS**: Usa el modo HDFS en el dashboard para visualizar datos sin descargas
3. **Explorar EDA**: Carga los archivos `correlation_matrix.parquet`, `descriptive_stats.parquet` o `chi_square_tests.parquet` para ver an√°lisis estad√≠stico completo
4. **Mapas interactivos**: Usa `regional.parquet` o `continental.parquet` para ver visualizaciones geogr√°ficas del mundo
5. **An√°lisis avanzado**: Usa PySpark dentro del processor para analizar los datos procesados
6. **Procesamiento en batch**: Crea scripts para procesar m√∫ltiples archivos autom√°ticamente
7. **Queries SQL con Spark**: Lee los Parquet desde HDFS y ejecuta queries SQL
8. **Machine Learning**: Entrena modelos usando los datos procesados en formato Parquet

## Flujo de Trabajo Completo

### üöÄ Inicio R√°pido (Setup + Procesamiento + Dashboard)

**Un solo comando para todo:**

### Windows (PowerShell)
```powershell
# 1. Levantar TODO (namenode + datanode + processor + dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (modo HDFS-first)
cd ..
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# 3. Abrir dashboard (ya est√° levantado)
# http://localhost:8501
```

### Linux/macOS (Bash)
```bash
# 1. Levantar TODO (namenode + datanode + processor + dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (modo HDFS-first)
cd ..
bash scripts/linux/process_full_dataset.sh --skip-download

# 3. Abrir dashboard (ya est√° levantado)
# http://localhost:8501
```

En el dashboard:
- Selecciona "HDFS (Recommended)"
- Host: `namenode`, Port: `9000`, Path: `/data/climaxtreme/processed`
- ¬°Listo! Ya puedes visualizar los 11 archivos Parquet

---

### üìã Setup Inicial (solo una vez)

```powershell
# Levantar todos los contenedores
cd infra
docker-compose up -d

# Esto levanta 4 contenedores:
#   - climaxtreme-namenode (HDFS NameNode)
#   - climaxtreme-datanode (HDFS DataNode)
#   - climaxtreme-processor (PySpark processor)
#   - climaxtreme-dashboard (Streamlit dashboard en puerto 8501)

# Esperar a que est√©n healthy (~30 segundos)
docker-compose ps

# Verificar que el dashboard est√© corriendo
docker logs climaxtreme-dashboard

# Volver al directorio ra√≠z
cd ..
```

---

### ‚öôÔ∏è Procesamiento Completo (m√©todo recomendado)

### Windows (PowerShell)
```powershell
# Opci√≥n 1: Todo autom√°tico (sube, procesa, descarga)
.\scripts\windows\process_full_dataset.ps1

# Opci√≥n 2: Solo HDFS (sin descarga local - RECOMENDADO)
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# Opci√≥n 3: Reprocesar (datos ya en HDFS)
.\scripts\windows\process_full_dataset.ps1 -SkipUpload
```

### Linux/macOS (Bash)
```bash
# Opci√≥n 1: Todo autom√°tico (sube, procesa, descarga)
bash scripts/linux/process_full_dataset.sh

# Opci√≥n 2: Solo HDFS (sin descarga local - RECOMENDADO)
bash scripts/linux/process_full_dataset.sh --skip-download

# Opci√≥n 3: Reprocesar (datos ya en HDFS)
bash scripts/linux/process_full_dataset.sh --skip-upload
```

**Tiempo estimado (dataset completo ~8.6M registros):**
- Upload a HDFS: ~2-5 min
- Procesamiento Spark: ~20-30 min
- Download (opcional): ~2-3 min
- **Total: ~25-40 minutos**

---

### üé® Visualizaci√≥n con Dashboard

```powershell
# El dashboard ya est√° corriendo si hiciste: docker-compose up -d
# Simplemente abre: http://localhost:8501

# Para iniciarlo manualmente:
cd infra
docker-compose up -d dashboard

# Ver logs del dashboard:
docker logs -f climaxtreme-dashboard

# Reiniciar dashboard (si haces cambios en el c√≥digo):
docker-compose restart dashboard

# En el navegador (http://localhost:8501):
#    - Seleccionar "HDFS (Recommended)" en sidebar
#    - Configurar: Host=namenode, Port=9000, Path=/data/climaxtreme/processed
#    - Seleccionar archivo para visualizar (11 opciones disponibles)
#    - Navegar por las 7 pesta√±as:
#      * Temperature Trends
#      * Heatmaps
#      * Seasonal Analysis
#      * Extreme Events
#      * Regional Analysis (con mapa interactivo del mundo)
#      * Continental Analysis (con mapa global)
#      * Exploratory Analysis (EDA: correlaciones, stats, chi-cuadrado)
```

---

### üîß Desarrollo (cuando modificas c√≥digo)

### Windows (PowerShell)
```powershell
# 1. Hacer cambios en Tools/src/climaxtreme/

# 2a. Si modificas procesamiento (preprocessing, ml, etc):
cd infra
docker-compose build processor
docker-compose restart processor

# 2b. Si modificas dashboard (dashboard/app.py):
docker-compose build dashboard
docker-compose restart dashboard

# 3. Reprocesar datos (sin re-upload)
cd ..
.\scripts\windows\process_full_dataset.ps1 -SkipUpload -SkipDownload

# 4. Verificar resultados
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# 5. Refrescar dashboard en el navegador (Ctrl+R)
```

### Linux/macOS (Bash)
```bash
# 1. Hacer cambios en Tools/src/climaxtreme/

# 2a. Si modificas procesamiento (preprocessing, ml, etc):
cd infra
docker-compose build processor
docker-compose restart processor

# 2b. Si modificas dashboard (dashboard/app.py):
docker-compose build dashboard
docker-compose restart dashboard

# 3. Reprocesar datos (sin re-upload)
cd ..
bash scripts/linux/process_full_dataset.sh --skip-upload --skip-download

# 4. Verificar resultados
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# 5. Refrescar dashboard en el navegador (Ctrl+R)
```

---

### üß™ Testing y Desarrollo R√°pido

Para desarrollar con muestras peque√±as:

### Windows (PowerShell)
```powershell
# 1. Cargar solo 100k filas (r√°pido)
.\scripts\windows\hdfs_setup_and_load.ps1 -CsvPath "DATA\GlobalLandTemperaturesByCity.csv" -Head 100000

# 2. Procesar manualmente
docker exec climaxtreme-processor climaxtreme preprocess `
  --input-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv" `
  --output-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed" `
  --format city-csv

# Tiempo: ~2-5 minutos (vs ~30 minutos para dataset completo)
```

### Linux/macOS (Bash)
```bash
# 1. Cargar solo 100k filas (r√°pido)
bash scripts/linux/hdfs_setup_and_load.sh --csv-path "DATA/GlobalLandTemperaturesByCity.csv" --head 100000

# 2. Procesar manualmente
docker exec climaxtreme-processor climaxtreme preprocess \
  --input-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv" \
  --output-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed" \
  --format city-csv

# Tiempo: ~2-5 minutos (vs ~30 minutos para dataset completo)
```

---

### üì¶ Gesti√≥n de Datos

```powershell
# Ver archivos en HDFS
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# Limpiar datos procesados (para reprocesar)
docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme/processed

# Limpiar todo (raw + processed)
docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme

# Descargar archivo espec√≠fico
docker exec climaxtreme-namenode hdfs dfs -get /data/climaxtreme/processed/monthly.parquet /tmp/
docker cp climaxtreme-namenode:/tmp/monthly.parquet ./DATA/processed/
```

---

### üõë Detener Sistema

```powershell
# Detener solo el dashboard
cd infra
docker-compose stop dashboard

# Detener contenedores (mantiene datos)
docker-compose stop

# Detener y eliminar contenedores (mantiene vol√∫menes)
docker-compose down

# Detener TODO y eliminar datos (‚ö†Ô∏è Cuidado!)
docker-compose down -v
```

## Recursos Adicionales

- **HDFS Web UI**: http://localhost:9870
- **Documentaci√≥n PySpark**: https://spark.apache.org/docs/latest/api/python/
- **Docker Compose Docs**: https://docs.docker.com/compose/
- **Documentaci√≥n Parquet**: Ver `PARQUETS.md` para detalles de estructura de archivos
- **Gu√≠a de EDA**: Ver `EDA_IMPLEMENTATION.md` para an√°lisis exploratorio
- **Mapas Interactivos**: Ver `MAPAS_INTERACTIVOS.md` para visualizaciones geogr√°ficas
- **Scripts README**: Ver `scripts/README.md` para documentaci√≥n detallada de scripts

---

## üìö Referencia R√°pida de Comandos

### Gesti√≥n de Contenedores

| Acci√≥n | Windows (PowerShell) | Linux/macOS (Bash) |
|--------|---------------------|-------------------|
| **Iniciar todo** | `cd infra; docker-compose up -d` | `cd infra && docker-compose up -d` |
| **Detener todo** | `cd infra; docker-compose down` | `cd infra && docker-compose down` |
| **Ver estado** | `.\scripts\windows\check_status.ps1` | `bash scripts/linux/check_status.sh` |
| **Ver logs** | `docker logs -f climaxtreme-namenode` | `docker logs -f climaxtreme-namenode` |

### Procesamiento

| Acci√≥n | Windows (PowerShell) | Linux/macOS (Bash) |
|--------|---------------------|-------------------|
| **Pipeline completo** | `.\scripts\windows\process_full_dataset.ps1` | `bash scripts/linux/process_full_dataset.sh` |
| **Sin descarga** | `.\scripts\windows\process_full_dataset.ps1 -SkipDownload` | `bash scripts/linux/process_full_dataset.sh --skip-download` |
| **Solo procesamiento** | `.\scripts\windows\process_full_dataset.ps1 -SkipUpload` | `bash scripts/linux/process_full_dataset.sh --skip-upload` |
| **Cargar muestra** | `.\scripts\windows\hdfs_setup_and_load.ps1 -Head 100000` | `bash scripts/linux/hdfs_setup_and_load.sh --head 100000` |

### HDFS

| Acci√≥n | Comando (Mismo en ambos SO) |
|--------|----------------------------|
| **Listar archivos** | `docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme` |
| **Ver procesados** | `docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed` |
| **Eliminar procesados** | `docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme/processed` |
| **Ver espacio usado** | `docker exec climaxtreme-namenode hdfs dfs -df -h` |

### Dashboard

| Acci√≥n | Comando (Mismo en ambos SO) |
|--------|----------------------------|
| **Iniciar dashboard** | `cd infra && docker-compose up -d dashboard` |
| **Ver logs** | `docker logs -f climaxtreme-dashboard` |
| **Reiniciar** | `docker-compose restart dashboard` |
| **Acceder** | http://localhost:8501 |

---

## üîç Soluci√≥n de Problemas Espec√≠ficos por SO

### Windows

**"No se puede ejecutar el script"**
```powershell
# Cambiar pol√≠tica de ejecuci√≥n (solo primera vez)
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

**Paths con espacios**
```powershell
# Usa comillas
.\scripts\windows\process_full_dataset.ps1 -CsvPath "C:\Mi Carpeta\archivo.csv"
```

### Linux/macOS

**"Permission denied"**
```bash
# Dar permisos de ejecuci√≥n
chmod +x scripts/linux/*.sh
```

**Scripts no usan colores**
```bash
# Aseg√∫rate de estar usando un terminal con soporte ANSI
# O ejecuta: export TERM=xterm-256color
```

**macOS: bc command not found**
```bash
# Instalar bc con Homebrew
brew install bc
```

## An√°lisis Exploratorio de Datos (EDA)

El sistema genera autom√°ticamente 3 archivos adicionales con an√°lisis estad√≠stico avanzado:

### 1. Matriz de Correlaci√≥n (correlation_matrix.parquet)

**Contenido:**
- Correlaciones de Pearson entre todas las variables num√©ricas
- Variables: `year`, `avg_temperature`, `min_temperature`, `max_temperature`, `temperature_range`

**Visualizaci√≥n en dashboard:**
- Heatmap interactivo con escala de colores RdBu_r
- Top 10 correlaciones m√°s fuertes (positivas y negativas)
- Valores de correlaci√≥n de -1 (negativa perfecta) a +1 (positiva perfecta)

**Ejemplo de uso:**
```powershell
# Cargar en dashboard
# 1. Seleccionar: correlation_matrix.parquet
# 2. Ir a Tab 7: "Exploratory Analysis (EDA)"
# 3. Ver heatmap de correlaciones
```

**Interpretaci√≥n:**
- |r| > 0.7: Correlaci√≥n fuerte
- |r| > 0.4: Correlaci√≥n moderada
- |r| < 0.3: Correlaci√≥n d√©bil

---

### 2. Estad√≠sticas Descriptivas (descriptive_stats.parquet)

**Contenido:**
- 11 estad√≠sticas por cada variable num√©rica
- M√©tricas: count, mean, std_dev, min, Q1, median, Q3, max, IQR, skewness, kurtosis

**Visualizaci√≥n en dashboard:**
- Tabla estilizada con gradiente de color
- Gr√°fico de barras con error bars (media ¬± desviaci√≥n est√°ndar)
- Box plots interactivos mostrando los 5 n√∫meros resumen

**Ejemplo de uso:**
```powershell
# Cargar en dashboard
# 1. Seleccionar: descriptive_stats.parquet
# 2. Ir a Tab 7: "Exploratory Analysis (EDA)"
# 3. Ver tabla y gr√°ficos de distribuci√≥n
```

**Interpretaci√≥n:**
- **Skewness < -1**: Distribuci√≥n sesgada izquierda
- **Skewness -0.5 a 0.5**: Aproximadamente sim√©trica
- **Skewness > 1**: Distribuci√≥n sesgada derecha
- **Kurtosis < 0**: Platic√∫rtica (colas ligeras)
- **Kurtosis ‚âà 0**: Mesoc√∫rtica (normal)
- **Kurtosis > 0**: Leptoc√∫rtica (colas pesadas)

---

### 3. Pruebas Chi-Cuadrado (chi_square_tests.parquet)

**Contenido:**
- Pruebas de independencia entre variables categ√≥ricas
- Tests: Continente vs Temperatura, Estaci√≥n vs Temperatura, Per√≠odo vs Temperatura
- Incluye: estad√≠stico œá¬≤, p-value, grados de libertad, significancia

**Visualizaci√≥n en dashboard:**
- Tabla de resultados con resaltado de tests significativos
- Gr√°fico de barras del estad√≠stico œá¬≤
- Interpretaci√≥n textual autom√°tica

**Ejemplo de uso:**
```powershell
# Cargar en dashboard
# 1. Seleccionar: chi_square_tests.parquet
# 2. Ir a Tab 7: "Exploratory Analysis (EDA)"
# 3. Ver resultados de pruebas de independencia
```

**Interpretaci√≥n:**
- **p-value < 0.05**: Variables dependientes (relaci√≥n significativa) ‚úÖ
- **p-value ‚â• 0.05**: Variables independientes (no hay relaci√≥n) ‚ùå

**Ejemplo de resultado:**
```
Test: Continent vs Temperature Category
œá¬≤ = 145,678.23
p-value = 0.0000 (< 0.05)
‚Üí Significativo: La temperatura depende del continente
```

---

### Casos de Uso del EDA

**1. An√°lisis Pre-Modelado:**
- Detectar multicolinealidad antes de entrenar modelos ML
- Identificar variables dependientes
- Verificar distribuciones para decidir transformaciones

**2. Validaci√≥n de Hip√≥tesis:**
- ¬øLas temperaturas est√°n aumentando? ‚Üí Ver correlaci√≥n `year` ‚Üî `avg_temperature`
- ¬øHay diferencias por continente? ‚Üí Ver chi-square continente vs temperatura
- ¬øLas estaciones afectan? ‚Üí Ver chi-square estaci√≥n vs temperatura

**3. Detecci√≥n de Anomal√≠as:**
- Usar IQR para detectar outliers: valores fuera de [Q1 - 1.5√óIQR, Q3 + 1.5√óIQR]
- Verificar skewness para distribuciones asim√©tricas
- Analizar kurtosis para eventos extremos

**4. Reportes Cient√≠ficos:**
- Exportar tablas de estad√≠sticas descriptivas
- Incluir heatmaps de correlaci√≥n
- Reportar resultados de tests estad√≠sticos

---

## Script Completo de Procesamiento

Para procesar el dataset completo y generar todos los 11 archivos Parquet autom√°ticamente:

```powershell
# Desde el directorio ra√≠z del proyecto
.\scripts\process_full_dataset.ps1
```

Este script ejecuta todo el pipeline:
1. ‚úÖ Verifica Docker
2. ‚úÖ Inicia HDFS (namenode + datanode)
3. ‚úÖ Sube el dataset completo a HDFS
4. ‚úÖ Procesa con PySpark generando 11 archivos:
   - 8 agregaciones clim√°ticas
   - 3 an√°lisis EDA
5. ‚úÖ (Opcional) Descarga resultados a local

**Par√°metros disponibles:**
```powershell
# Procesar todo y descargar
.\scripts\process_full_dataset.ps1

# Solo procesar (datos quedan en HDFS)
.\scripts\process_full_dataset.ps1 -SkipDownload

# Solo procesar (asume datos ya en HDFS)
.\scripts\process_full_dataset.ps1 -SkipUpload

# Solo procesar sin upload ni download
.\scripts\process_full_dataset.ps1 -SkipUpload -SkipDownload
```

**Tiempo estimado (dataset completo ~8.6M registros):**
- Upload a HDFS: ~2-5 minutos
- Procesamiento Spark: ~20-30 minutos
  - Agregaciones: ~15 minutos
  - EDA: ~10-15 minutos adicionales
- Download (opcional): ~2-3 minutos
- **Total: ~25-40 minutos**

**Archivos generados (11 total, ~150 MB):**
```
/data/climaxtreme/processed/
‚îú‚îÄ‚îÄ monthly.parquet              (~80 MB)
‚îú‚îÄ‚îÄ yearly.parquet               (~30 MB)
‚îú‚îÄ‚îÄ anomalies.parquet            (~15 MB)
‚îú‚îÄ‚îÄ climatology.parquet          (~10 MB)
‚îú‚îÄ‚îÄ seasonal.parquet             (~8 MB)
‚îú‚îÄ‚îÄ extreme_thresholds.parquet   (~5 MB)
‚îú‚îÄ‚îÄ regional.parquet             (~1 MB)
‚îú‚îÄ‚îÄ continental.parquet          (~500 KB)
‚îú‚îÄ‚îÄ correlation_matrix.parquet   (<1 MB)
‚îú‚îÄ‚îÄ descriptive_stats.parquet    (<1 MB)
‚îî‚îÄ‚îÄ chi_square_tests.parquet     (<1 MB)
```
