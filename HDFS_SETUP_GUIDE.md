# GuÃ­a de ConfiguraciÃ³n HDFS para climaXtreme

> **Nota**: Esta guÃ­a soporta **Windows (PowerShell)** y **Linux/macOS (Bash)**. Los comandos especÃ­ficos estÃ¡n claramente marcados.

## ï¿½ Selecciona tu Sistema Operativo

Esta guÃ­a proporciona comandos para ambas plataformas:

| Sistema Operativo | Shell | Scripts ubicados en |
|-------------------|-------|---------------------|
| **Windows** | PowerShell | `scripts/windows/` |
| **Linux/macOS** | Bash | `scripts/linux/` |

### Diferencias Clave

| CaracterÃ­stica | Windows | Linux/macOS |
|----------------|---------|-------------|
| **Scripts** | `.ps1` (PowerShell) | `.sh` (Bash) |
| **Paths** | `\` (backslash) | `/` (forward slash) |
| **Flags** | `-ParameterName` | `--parameter-name` |
| **EjecuciÃ³n** | `.\script.ps1` | `bash script.sh` o `./script.sh` |

### ConfiguraciÃ³n Inicial

**Windows**: No requiere configuraciÃ³n adicional (PowerShell estÃ¡ incluido)

**Linux/macOS**: Dar permisos de ejecuciÃ³n a los scripts (solo primera vez)
```bash
chmod +x scripts/linux/*.sh
```

---

## ï¿½ğŸš€ Quick Start (2 comandos)

### Windows (PowerShell)
```powershell
# 1. Levantar todos los contenedores (HDFS + Dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (automÃ¡tico)
cd ..
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# 3. Abrir dashboard: http://localhost:8501
```

### Linux/macOS (Bash)
```bash
# 1. Levantar todos los contenedores (HDFS + Dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (automÃ¡tico)
cd ..
bash scripts/linux/process_full_dataset.sh --skip-download

# 3. Abrir dashboard: http://localhost:8501
```

**Resultado**: Abre http://localhost:8501 â†’ Selecciona "HDFS" en el sidebar â†’ Â¡Listo! ğŸ‰

**Tiempo total: ~30-40 minutos** (la mayor parte es procesamiento automÃ¡tico en Docker)

**Nota:** Todo se ejecuta en **Docker** (HDFS + Processor + Dashboard). El dashboard ya estÃ¡ levantado con `docker-compose up -d`.

---

## Requisitos Previos

1. **Docker Desktop** instalado y en ejecuciÃ³n (WSL2 recomendado)
2. **Dataset** en `DATA/GlobalLandTemperaturesByCity.csv`

**NOTA**: Todo el procesamiento se realiza dentro de contenedores Docker. No necesitas instalar Python, PySpark ni Java en tu mÃ¡quina local.

## Paso 1: Verificar Docker Desktop

Antes de ejecutar cualquier comando, asegÃºrate que Docker Desktop estÃ¡ corriendo:

```powershell
docker info
```

Si ves un error, abre Docker Desktop y espera a que muestre "Running" en verde.

## Paso 2: Iniciar Contenedores HDFS y Processor

Desde el directorio `infra` en PowerShell:

```powershell
cd infra
docker-compose up -d
```

Esto iniciarÃ¡ **3 contenedores**:
- **climaxtreme-namenode**: HDFS NameNode (puerto 9870 para UI, 9000 para datos)
- **climaxtreme-datanode**: HDFS DataNode
- **climaxtreme-processor**: Contenedor con Python 3.9, PySpark y Java 17 para procesamiento

**Tiempo esperado**: 
- Primera vez: 5-10 minutos (descarga de imÃ¡genes Docker + build del processor)
- Siguientes veces: 10-30 segundos

### Verificar que los contenedores estÃ¡n corriendo:

```powershell
docker-compose ps
```

DeberÃ­as ver los 3 contenedores en estado "Running" o "Up", y namenode/datanode con "(healthy)".

## Paso 3: Procesamiento Completo AutomÃ¡tico (Recomendado)

**Usa el script unificado que hace TODO automÃ¡ticamente:**

### Windows (PowerShell)
```powershell
# Desde el directorio raÃ­z del proyecto
.\scripts\windows\process_full_dataset.ps1
```

### Linux/macOS (Bash)
```bash
# Desde el directorio raÃ­z del proyecto
bash scripts/linux/process_full_dataset.sh
```

Este script ejecuta el **pipeline completo**:
1. âœ… Verifica que Docker estÃ© corriendo
2. âœ… Inicia contenedores HDFS (namenode + datanode + processor)
3. âœ… Sube el dataset completo a HDFS
4. âœ… Ejecuta procesamiento Spark (genera 11 archivos Parquet)
5. âœ… (Opcional) Descarga resultados a `DATA/processed/`

**ParÃ¡metros disponibles:**

### Windows (PowerShell)
```powershell
# Procesar todo (default: sube, procesa, y descarga)
.\scripts\windows\process_full_dataset.ps1

# Procesar sin descargar (datos quedan solo en HDFS - RECOMENDADO)
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# Solo procesar (asume que datos ya estÃ¡n en HDFS)
.\scripts\windows\process_full_dataset.ps1 -SkipUpload

# Solo procesar (sin upload ni download)
.\scripts\windows\process_full_dataset.ps1 -SkipUpload -SkipDownload

# Especificar ruta del CSV
.\scripts\windows\process_full_dataset.ps1 -CsvPath "DATA\MiArchivo.csv"
```

### Linux/macOS (Bash)
```bash
# Procesar todo (default: sube, procesa, y descarga)
bash scripts/linux/process_full_dataset.sh

# Procesar sin descargar (datos quedan solo en HDFS - RECOMENDADO)
bash scripts/linux/process_full_dataset.sh --skip-download

# Solo procesar (asume que datos ya estÃ¡n en HDFS)
bash scripts/linux/process_full_dataset.sh --skip-upload

# Solo procesar (sin upload ni download)
bash scripts/linux/process_full_dataset.sh --skip-upload --skip-download

# Especificar ruta del CSV
bash scripts/linux/process_full_dataset.sh --csv-path "DATA/MiArchivo.csv"
```

**Salida esperada:**
```
========================================
  climaXtreme - Procesamiento Completo
========================================

PASO 1/4: Cargando dataset COMPLETO a HDFS...
âœ“ Dataset cargado exitosamente a HDFS

PASO 2/4: Ejecutando procesamiento con PySpark...
âœ“ Procesamiento completado (11 archivos generados)

PASO 3/4: Verificando archivos en HDFS...
âœ“ Todos los archivos Parquet verificados

PASO 4/4: Descargando resultados...
  âœ“ monthly.parquet descargado
  âœ“ yearly.parquet descargado
  ... (11 archivos total)

=========================================
  âœ“ PROCESAMIENTO COMPLETADO
=========================================
```

**Archivos generados en HDFS:**
- `/data/climaxtreme/processed/monthly.parquet`
- `/data/climaxtreme/processed/yearly.parquet`
- `/data/climaxtreme/processed/anomalies.parquet`
- `/data/climaxtreme/processed/climatology.parquet`
- `/data/climaxtreme/processed/seasonal.parquet`
- `/data/climaxtreme/processed/extreme_thresholds.parquet`
- `/data/climaxtreme/processed/regional.parquet`
- `/data/climaxtreme/processed/continental.parquet`
- `/data/climaxtreme/processed/correlation_matrix.parquet`
- `/data/climaxtreme/processed/descriptive_stats.parquet`
- `/data/climaxtreme/processed/chi_square_tests.parquet`

---

## Paso 3 Alternativo: Carga Manual (Solo para desarrollo)

Si necesitas cargar datos manualmente sin procesamiento:

### Windows (PowerShell)
```powershell
# Cargar solo una muestra (100,000 filas)
.\scripts\windows\hdfs_setup_and_load.ps1 -CsvPath "DATA\GlobalLandTemperaturesByCity.csv" -Head 100000

# Cargar archivo completo
.\scripts\windows\hdfs_setup_and_load.ps1 -CsvPath "DATA\GlobalLandTemperaturesByCity.csv" -FullFile
```

**ParÃ¡metros del script manual (Windows)**:
- `-CsvPath`: Ruta al archivo CSV original
- `-Head`: NÃºmero de filas a cargar (para muestras)
- `-FullFile`: Cargar archivo completo (sin lÃ­mite de filas)

### Linux/macOS (Bash)
```bash
# Cargar solo una muestra (100,000 filas)
bash scripts/linux/hdfs_setup_and_load.sh --csv-path "DATA/GlobalLandTemperaturesByCity.csv" --head 100000

# Cargar archivo completo
bash scripts/linux/hdfs_setup_and_load.sh --csv-path "DATA/GlobalLandTemperaturesByCity.csv" --full-file
```

**ParÃ¡metros del script manual (Linux/macOS)**:
- `--csv-path`: Ruta al archivo CSV original
- `--head`: NÃºmero de filas a cargar (para muestras)
- `--full-file`: Cargar archivo completo (sin lÃ­mite de filas)

## Paso 4: Verificar HDFS

### OpciÃ³n A: UI Web
Abre en tu navegador: http://localhost:9870

Ve a "Utilities" â†’ "Browse the file system" â†’ Navega a `/data/climaxtreme`

### OpciÃ³n B: LÃ­nea de comandos
```powershell
# Ver archivos raw (CSV original)
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme

# Ver archivos procesados (Parquet)
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# Ver detalles de un archivo especÃ­fico
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed/monthly.parquet

# Ver primeras lÃ­neas del CSV
docker exec climaxtreme-namenode hdfs dfs -tail /data/climaxtreme/GlobalLandTemperaturesByCity.csv
```

---

## Paso 5: Lanzar Dashboard de Streamlit

**NOTA**: Para usar el dashboard necesitas instalar el paquete climaxtreme en tu mÃ¡quina local.

### InstalaciÃ³n local (solo para el dashboard):

```powershell
# Desde el directorio Tools
cd Tools
pip install -e .
```

### Ejecutar dashboard:

```powershell
# El dashboard se ejecuta en DOCKER (contenedor dedicado)
# Ya estÃ¡ levantado si hiciste: docker-compose up -d

# Para iniciarlo por separado:
cd infra
docker-compose up -d dashboard

# Para ver logs del dashboard:
docker logs -f climaxtreme-dashboard
```

Abre: http://localhost:8501

### Configurar fuente de datos:

En el **sidebar del dashboard** verÃ¡s un selector de fuente de datos:

**OpciÃ³n 1 - HDFS (Recomendado para Big Data):**
1. Seleccionar: **HDFS (Recommended)**
2. Configurar:
   - HDFS Host: `namenode`
   - HDFS Port: `9000`
   - HDFS Base Path: `/data/climaxtreme/processed`
3. El dashboard leerÃ¡ directo desde HDFS sin descargar archivos

**Archivos disponibles para visualizar:**
- **monthly.parquet**: AnÃ¡lisis de tendencias mensuales
- **yearly.parquet**: AnÃ¡lisis de tendencias anuales con lÃ­nea de tendencia
- **anomalies.parquet**: DetecciÃ³n de anomalÃ­as climÃ¡ticas
- **seasonal.parquet**: Patrones estacionales
- **extreme_thresholds.parquet**: Umbrales de eventos extremos
- **regional.parquet**: AnÃ¡lisis por regiÃ³n geogrÃ¡fica (16 regiones) + mapa interactivo ğŸ—ºï¸
- **continental.parquet**: AnÃ¡lisis por continente (7 continentes) + mapa global ğŸŒ
- **correlation_matrix.parquet**: Matriz de correlaciÃ³n de Pearson (EDA) ğŸ“Š
- **descriptive_stats.parquet**: EstadÃ­sticas descriptivas completas (EDA) ğŸ“ˆ
- **chi_square_tests.parquet**: Pruebas de independencia Chi-cuadrado (EDA) ğŸ§ª

**PestaÃ±as del dashboard (7 en total):**
1. ğŸŒ¡ï¸ **Temperature Trends**: Tendencias de temperatura a lo largo del tiempo
2. ğŸ—ºï¸ **Heatmaps**: Mapas de calor de temperatura
3. ğŸ“ˆ **Seasonal Analysis**: AnÃ¡lisis estacional
4. âš¡ **Extreme Events**: Eventos climÃ¡ticos extremos
5. ğŸŒ **Regional Analysis**: AnÃ¡lisis por regiÃ³n + mapa mundial interactivo
6. ğŸŒ **Continental Analysis**: AnÃ¡lisis por continente + mapa global con burbujas
7. ğŸ“Š **Exploratory Analysis (EDA)**: Correlaciones, estadÃ­sticas descriptivas y pruebas Chi-cuadrado

**OpciÃ³n 2 - Local Files (Para desarrollo/demos):**
1. Seleccionar: **Local Files**
2. Primero descarga los archivos desde HDFS (ver secciÃ³n de comandos Ãºtiles)
3. El dashboard leerÃ¡ desde `DATA/processed/`

**Ventajas del modo HDFS:**
- âœ… Sin descargas innecesarias
- âœ… HDFS como Ãºnica fuente de verdad (principio Big Data)
- âœ… Siempre datos actualizados
- âœ… Ahorro de espacio en disco local
- âœ… Acceso a todos los 11 archivos Parquet generados
- âœ… Visualizaciones interactivas (mapas, heatmaps, estadÃ­sticas)

## Arquitectura del Sistema

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Docker Network (hdfs)                 â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚  NameNode    â”‚â—„â”€â”€â”€â”€â–ºâ”‚  DataNode    â”‚                â”‚
â”‚  â”‚  (Port 9870) â”‚      â”‚              â”‚                â”‚
â”‚  â”‚  (Port 9000) â”‚      â”‚              â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚         â–²                                                â”‚
â”‚         â”‚                                                â”‚
â”‚         â”‚ HDFS Protocol                                 â”‚
â”‚         â”‚                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚  â”‚       Processor Container           â”‚                â”‚
â”‚  â”‚  - Python 3.9                       â”‚                â”‚
â”‚  â”‚  - PySpark 3.4+                     â”‚                â”‚
â”‚  â”‚  - Java 17 (OpenJDK)                â”‚                â”‚
â”‚  â”‚  - climaxtreme package              â”‚                â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â–²
         â”‚ docker exec
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚ Windows  â”‚
    â”‚ PowerShellâ”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## SoluciÃ³n de Problemas

### Error: "image not found" o build muy lento
**Problema**: Docker no puede descargar las imÃ¡genes o el build del processor es lento.

**SoluciÃ³n**:
1. Verifica tu conexiÃ³n a internet
2. La primera vez, el build del processor descarga muchas dependencias Python (~500MB)
3. Si estÃ¡s detrÃ¡s de un proxy corporativo, configÃºralo en Docker Desktop â†’ Settings â†’ Resources â†’ Proxies

### Error: "Unable to load native-hadoop library"
**Problema**: Advertencia al ejecutar PySpark (no es crÃ­tico).

**SoluciÃ³n**: Es solo una advertencia, el procesamiento funciona correctamente. Hadoop usa librerÃ­as Java en su lugar.

### Error: "TypeError: bad operand type for abs(): 'Column'"
**Problema**: VersiÃ³n antigua del cÃ³digo en el contenedor.

**SoluciÃ³n**: Reconstruir la imagen del processor:
```powershell
cd infra
docker-compose build processor
docker-compose stop processor
docker-compose up -d processor
```

### Error: "Connection refused" al conectar con HDFS
**Problema**: Intentas ejecutar `climaxtreme` desde tu mÃ¡quina local en lugar del contenedor.

**SoluciÃ³n**: **SIEMPRE** ejecuta los comandos de procesamiento dentro del contenedor:
```powershell
docker exec climaxtreme-processor climaxtreme preprocess ...
```

NO ejecutes directamente:
```powershell
# âŒ INCORRECTO - No funcionarÃ¡ desde Windows
climaxtreme preprocess --input-path "hdfs://..."

# âœ… CORRECTO - Ejecutar dentro del contenedor
docker exec climaxtreme-processor climaxtreme preprocess --input-path "hdfs://..."
```

### Error: "The system cannot find the file specified"
**Problema**: Docker Desktop no estÃ¡ corriendo.

**SoluciÃ³n**: Abre Docker Desktop y espera a que estÃ© en estado "Running"

### Error: "No se pudo iniciar el contenedor" o contenedores no saludables
**Problema**: Los contenedores no arrancan correctamente.

**SoluciÃ³n**:
1. Ver logs de los contenedores:
   ```powershell
   docker logs climaxtreme-namenode
   docker logs climaxtreme-datanode
   docker logs climaxtreme-processor
   ```
2. Verificar el estado:
   ```powershell
   cd infra
   docker-compose ps
   ```
3. Reiniciar desde cero:
   ```powershell
   docker-compose down -v
   docker-compose up -d
   ```

### Los contenedores se detienen despuÃ©s de unos segundos
**Problema**: El processor estÃ¡ configurado para mantenerse corriendo indefinidamente.

**SoluciÃ³n**: Si el processor se detiene, verifica los logs:
```powershell
docker logs climaxtreme-processor
```

El contenedor debe ejecutar `tail -f /dev/null` para mantenerse activo.

### El dashboard no carga los datos
**Problema**: No se ven archivos disponibles en el dashboard.

**SoluciÃ³n**: 
- **Modo HDFS**: Verifica que los contenedores estÃ©n corriendo y que los archivos existan en HDFS:
  ```powershell
  docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed
  ```
- **Modo Local Files**: Verifica que `DATA/processed/` contenga archivos `.parquet`. Si no existen, descÃ¡rgalos desde HDFS:
  ```powershell
  docker exec climaxtreme-namenode hdfs dfs -get /data/climaxtreme/processed/*.parquet /tmp/
  docker cp climaxtreme-namenode:/tmp/monthly.parquet ./DATA/processed/
  ```

### Error: "Could not connect to HDFS" en el dashboard
**Problema**: El dashboard no puede conectarse al namenode.

**SoluciÃ³n**:
1. Verifica que los contenedores estÃ©n corriendo:
   ```powershell
   docker ps | Select-String "namenode"
   ```
2. Verifica la configuraciÃ³n en el sidebar:
   - HDFS Host debe ser: `namenode` (o `localhost` si estÃ¡s en Windows)
   - HDFS Port debe ser: `9000`
3. Si usas `localhost` como host, puede que necesites usar la IP del contenedor:
   ```powershell
   docker inspect climaxtreme-namenode -f '{{.NetworkSettings.Networks.hdfs.IPAddress}}'
   ```

## Detener HDFS

```powershell
cd infra
docker-compose down
```

Para eliminar tambiÃ©n los volÃºmenes (datos persistentes):
```powershell
docker-compose down -v
```

**NOTA**: Si ejecutas `down -v`, perderÃ¡s todos los datos en HDFS y tendrÃ¡s que volver a cargarlos.

## Comandos Ãštiles

### Docker & Contenedores

```powershell
# Ver todos los contenedores corriendo
docker ps

# Ver logs en tiempo real
docker logs -f climaxtreme-namenode
docker logs -f climaxtreme-processor

# Entrar al contenedor (bash interactivo)
docker exec -it climaxtreme-namenode bash
docker exec -it climaxtreme-processor bash

# Reiniciar un contenedor especÃ­fico
cd infra
docker-compose restart processor
```

### HDFS (ejecutar desde cualquier lugar)

```powershell
# Listar archivos en HDFS
docker exec climaxtreme-namenode hdfs dfs -ls /
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme

# Ver las Ãºltimas lÃ­neas de un archivo
docker exec climaxtreme-namenode hdfs dfs -tail /data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv

# Ver espacio usado en HDFS
docker exec climaxtreme-namenode hdfs dfs -df -h

# Descargar archivo desde HDFS a tu mÃ¡quina (solo si usas modo Local Files)
docker exec climaxtreme-namenode hdfs dfs -get /data/climaxtreme/processed/monthly.parquet /tmp/
docker cp climaxtreme-namenode:/tmp/monthly.parquet ./

# NOTA: Con modo HDFS en el dashboard, NO necesitas descargar archivos

# Eliminar archivo/directorio en HDFS
docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme/processed
```

### Procesamiento (ejecutar desde cualquier lugar)

```powershell
# Ejecutar preprocesamiento
docker exec climaxtreme-processor climaxtreme preprocess `
  --input-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv" `
  --output-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed" `
  --format city-csv

# Ver la ayuda del CLI
docker exec climaxtreme-processor climaxtreme --help
docker exec climaxtreme-processor climaxtreme preprocess --help

# Ejecutar Python interactivo dentro del processor
docker exec -it climaxtreme-processor python3
```

### Reconstruir el Processor (cuando cambias cÃ³digo)

```powershell
cd infra
docker-compose build processor
docker-compose stop processor
docker-compose up -d processor
```

## PrÃ³ximos Pasos

1. **AÃ±adir mÃ¡s datos**: Modifica el parÃ¡metro `-Head` del script para cargar mÃ¡s filas, o elimÃ­nalo para cargar todo el dataset
2. **Dashboard con HDFS**: Usa el modo HDFS en el dashboard para visualizar datos sin descargas
3. **Explorar EDA**: Carga los archivos `correlation_matrix.parquet`, `descriptive_stats.parquet` o `chi_square_tests.parquet` para ver anÃ¡lisis estadÃ­stico completo
4. **Mapas interactivos**: Usa `regional.parquet` o `continental.parquet` para ver visualizaciones geogrÃ¡ficas del mundo
5. **AnÃ¡lisis avanzado**: Usa PySpark dentro del processor para analizar los datos procesados
6. **Procesamiento en batch**: Crea scripts para procesar mÃºltiples archivos automÃ¡ticamente
7. **Queries SQL con Spark**: Lee los Parquet desde HDFS y ejecuta queries SQL
8. **Machine Learning**: Entrena modelos usando los datos procesados en formato Parquet

## Flujo de Trabajo Completo

### ğŸš€ Inicio RÃ¡pido (Setup + Procesamiento + Dashboard)

**Un solo comando para todo:**

### Windows (PowerShell)
```powershell
# 1. Levantar TODO (namenode + datanode + processor + dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (modo HDFS-first)
cd ..
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# 3. Abrir dashboard (ya estÃ¡ levantado)
# http://localhost:8501
```

### Linux/macOS (Bash)
```bash
# 1. Levantar TODO (namenode + datanode + processor + dashboard)
cd infra
docker-compose up -d

# 2. Procesar dataset completo (modo HDFS-first)
cd ..
bash scripts/linux/process_full_dataset.sh --skip-download

# 3. Abrir dashboard (ya estÃ¡ levantado)
# http://localhost:8501
```

En el dashboard:
- Selecciona "HDFS (Recommended)"
- Host: `namenode`, Port: `9000`, Path: `/data/climaxtreme/processed`
- Â¡Listo! Ya puedes visualizar los 11 archivos Parquet

---

### ğŸ“‹ Setup Inicial (solo una vez)

```powershell
# Levantar todos los contenedores
cd infra
docker-compose up -d

# Esto levanta 4 contenedores:
#   - climaxtreme-namenode (HDFS NameNode)
#   - climaxtreme-datanode (HDFS DataNode)
#   - climaxtreme-processor (PySpark processor)
#   - climaxtreme-dashboard (Streamlit dashboard en puerto 8501)

# Esperar a que estÃ©n healthy (~30 segundos)
docker-compose ps

# Verificar que el dashboard estÃ© corriendo
docker logs climaxtreme-dashboard

# Volver al directorio raÃ­z
cd ..
```

---

### âš™ï¸ Procesamiento Completo (mÃ©todo recomendado)

### Windows (PowerShell)
```powershell
# OpciÃ³n 1: Todo automÃ¡tico (sube, procesa, descarga)
.\scripts\windows\process_full_dataset.ps1

# OpciÃ³n 2: Solo HDFS (sin descarga local - RECOMENDADO)
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# OpciÃ³n 3: Reprocesar (datos ya en HDFS)
.\scripts\windows\process_full_dataset.ps1 -SkipUpload
```

### Linux/macOS (Bash)
```bash
# OpciÃ³n 1: Todo automÃ¡tico (sube, procesa, descarga)
bash scripts/linux/process_full_dataset.sh

# OpciÃ³n 2: Solo HDFS (sin descarga local - RECOMENDADO)
bash scripts/linux/process_full_dataset.sh --skip-download

# OpciÃ³n 3: Reprocesar (datos ya en HDFS)
bash scripts/linux/process_full_dataset.sh --skip-upload
```

**Tiempo estimado (dataset completo ~8.6M registros):**
- Upload a HDFS: ~2-5 min
- Procesamiento Spark: ~20-30 min
- Download (opcional): ~2-3 min
- **Total: ~25-40 minutos**

---

### ğŸ¨ VisualizaciÃ³n con Dashboard

```powershell
# El dashboard ya estÃ¡ corriendo si hiciste: docker-compose up -d
# Simplemente abre: http://localhost:8501

# Para iniciarlo manualmente:
cd infra
docker-compose up -d dashboard

# Ver logs del dashboard:
docker logs -f climaxtreme-dashboard

# Reiniciar dashboard (si haces cambios en el cÃ³digo):
docker-compose restart dashboard

# En el navegador (http://localhost:8501):
#    - Seleccionar "HDFS (Recommended)" en sidebar
#    - Configurar: Host=namenode, Port=9000, Path=/data/climaxtreme/processed
#    - Seleccionar archivo para visualizar (11 opciones disponibles)
#    - Navegar por las 7 pestaÃ±as:
#      * Temperature Trends
#      * Heatmaps
#      * Seasonal Analysis
#      * Extreme Events
#      * Regional Analysis (con mapa interactivo del mundo)
#      * Continental Analysis (con mapa global)
#      * Exploratory Analysis (EDA: correlaciones, stats, chi-cuadrado)
```

---

### ğŸ”§ Desarrollo (cuando modificas cÃ³digo)

### Windows (PowerShell)
```powershell
# 1. Hacer cambios en Tools/src/climaxtreme/

# 2a. Si modificas procesamiento (preprocessing, ml, etc):
cd infra
docker-compose build processor
docker-compose restart processor

# 2b. Si modificas dashboard (dashboard/app.py):
docker-compose build dashboard
docker-compose restart dashboard

# 3. Reprocesar datos (sin re-upload)
cd ..
.\scripts\windows\process_full_dataset.ps1 -SkipUpload -SkipDownload

# 4. Verificar resultados
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# 5. Refrescar dashboard en el navegador (Ctrl+R)
```

### Linux/macOS (Bash)
```bash
# 1. Hacer cambios en Tools/src/climaxtreme/

# 2a. Si modificas procesamiento (preprocessing, ml, etc):
cd infra
docker-compose build processor
docker-compose restart processor

# 2b. Si modificas dashboard (dashboard/app.py):
docker-compose build dashboard
docker-compose restart dashboard

# 3. Reprocesar datos (sin re-upload)
cd ..
bash scripts/linux/process_full_dataset.sh --skip-upload --skip-download

# 4. Verificar resultados
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# 5. Refrescar dashboard en el navegador (Ctrl+R)
```

---

### ğŸ§ª Testing y Desarrollo RÃ¡pido

Para desarrollar con muestras pequeÃ±as:

### Windows (PowerShell)
```powershell
# 1. Cargar solo 100k filas (rÃ¡pido)
.\scripts\windows\hdfs_setup_and_load.ps1 -CsvPath "DATA\GlobalLandTemperaturesByCity.csv" -Head 100000

# 2. Procesar manualmente
docker exec climaxtreme-processor climaxtreme preprocess `
  --input-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv" `
  --output-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed" `
  --format city-csv

# Tiempo: ~2-5 minutos (vs ~30 minutos para dataset completo)
```

### Linux/macOS (Bash)
```bash
# 1. Cargar solo 100k filas (rÃ¡pido)
bash scripts/linux/hdfs_setup_and_load.sh --csv-path "DATA/GlobalLandTemperaturesByCity.csv" --head 100000

# 2. Procesar manualmente
docker exec climaxtreme-processor climaxtreme preprocess \
  --input-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv" \
  --output-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed" \
  --format city-csv

# Tiempo: ~2-5 minutos (vs ~30 minutos para dataset completo)
```

---

### ğŸ“¦ GestiÃ³n de Datos

```powershell
# Ver archivos en HDFS
docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed

# Limpiar datos procesados (para reprocesar)
docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme/processed

# Limpiar todo (raw + processed)
docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme

# Descargar archivo especÃ­fico
docker exec climaxtreme-namenode hdfs dfs -get /data/climaxtreme/processed/monthly.parquet /tmp/
docker cp climaxtreme-namenode:/tmp/monthly.parquet ./DATA/processed/
```

---

### ğŸ›‘ Detener Sistema

```powershell
# Detener solo el dashboard
cd infra
docker-compose stop dashboard

# Detener contenedores (mantiene datos)
docker-compose stop

# Detener y eliminar contenedores (mantiene volÃºmenes)
docker-compose down

# Detener TODO y eliminar datos (âš ï¸ Cuidado!)
docker-compose down -v
```

## Recursos Adicionales

- **HDFS Web UI**: http://localhost:9870
- **DocumentaciÃ³n PySpark**: https://spark.apache.org/docs/latest/api/python/
- **Docker Compose Docs**: https://docs.docker.com/compose/
- **DocumentaciÃ³n Parquet**: Ver `PARQUETS.md` para detalles de estructura de archivos
- **GuÃ­a de EDA**: Ver `EDA_IMPLEMENTATION.md` para anÃ¡lisis exploratorio
- **Mapas Interactivos**: Ver `MAPAS_INTERACTIVOS.md` para visualizaciones geogrÃ¡ficas
- **Scripts README**: Ver `scripts/README.md` para documentaciÃ³n detallada de scripts

---

## ğŸ“š Referencia RÃ¡pida de Comandos

### GestiÃ³n de Contenedores

| AcciÃ³n | Windows (PowerShell) | Linux/macOS (Bash) |
|--------|---------------------|-------------------|
| **Iniciar todo** | `cd infra; docker-compose up -d` | `cd infra && docker-compose up -d` |
| **Detener todo** | `cd infra; docker-compose down` | `cd infra && docker-compose down` |
| **Ver estado** | `.\scripts\windows\check_status.ps1` | `bash scripts/linux/check_status.sh` |
| **Ver logs** | `docker logs -f climaxtreme-namenode` | `docker logs -f climaxtreme-namenode` |

### Procesamiento

| AcciÃ³n | Windows (PowerShell) | Linux/macOS (Bash) |
|--------|---------------------|-------------------|
| **Pipeline completo** | `.\scripts\windows\process_full_dataset.ps1` | `bash scripts/linux/process_full_dataset.sh` |
| **Sin descarga** | `.\scripts\windows\process_full_dataset.ps1 -SkipDownload` | `bash scripts/linux/process_full_dataset.sh --skip-download` |
| **Solo procesamiento** | `.\scripts\windows\process_full_dataset.ps1 -SkipUpload` | `bash scripts/linux/process_full_dataset.sh --skip-upload` |
| **Cargar muestra** | `.\scripts\windows\hdfs_setup_and_load.ps1 -Head 100000` | `bash scripts/linux/hdfs_setup_and_load.sh --head 100000` |

### HDFS

| AcciÃ³n | Comando (Mismo en ambos SO) |
|--------|----------------------------|
| **Listar archivos** | `docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme` |
| **Ver procesados** | `docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme/processed` |
| **Eliminar procesados** | `docker exec climaxtreme-namenode hdfs dfs -rm -r /data/climaxtreme/processed` |
| **Ver espacio usado** | `docker exec climaxtreme-namenode hdfs dfs -df -h` |

### Dashboard

| AcciÃ³n | Comando (Mismo en ambos SO) |
|--------|----------------------------|
| **Iniciar dashboard** | `cd infra && docker-compose up -d dashboard` |
| **Ver logs** | `docker logs -f climaxtreme-dashboard` |
| **Reiniciar** | `docker-compose restart dashboard` |
| **Acceder** | http://localhost:8501 |

---

## ğŸ” SoluciÃ³n de Problemas EspecÃ­ficos por SO

### Windows

**"No se puede ejecutar el script"**
```powershell
# Cambiar polÃ­tica de ejecuciÃ³n (solo primera vez)
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

**Paths con espacios**
```powershell
# Usa comillas
.\scripts\windows\process_full_dataset.ps1 -CsvPath "C:\Mi Carpeta\archivo.csv"
```

### Linux/macOS

**"Permission denied"**
```bash
# Dar permisos de ejecuciÃ³n
chmod +x scripts/linux/*.sh
```

**Scripts no usan colores**
```bash
# AsegÃºrate de estar usando un terminal con soporte ANSI
# O ejecuta: export TERM=xterm-256color
```

**macOS: bc command not found**
```bash
# Instalar bc con Homebrew
brew install bc
```

## AnÃ¡lisis Exploratorio de Datos (EDA)

El sistema genera automÃ¡ticamente 3 archivos adicionales con anÃ¡lisis estadÃ­stico avanzado:

### 1. Matriz de CorrelaciÃ³n (correlation_matrix.parquet)

**Contenido:**
- Correlaciones de Pearson entre todas las variables numÃ©ricas
- Variables: `year`, `avg_temperature`, `min_temperature`, `max_temperature`, `temperature_range`

**VisualizaciÃ³n en dashboard:**
- Heatmap interactivo con escala de colores RdBu_r
- Top 10 correlaciones mÃ¡s fuertes (positivas y negativas)
- Valores de correlaciÃ³n de -1 (negativa perfecta) a +1 (positiva perfecta)

**Ejemplo de uso:**
```powershell
# Cargar en dashboard
# 1. Seleccionar: correlation_matrix.parquet
# 2. Ir a Tab 7: "Exploratory Analysis (EDA)"
# 3. Ver heatmap de correlaciones
```

**InterpretaciÃ³n:**
- |r| > 0.7: CorrelaciÃ³n fuerte
- |r| > 0.4: CorrelaciÃ³n moderada
- |r| < 0.3: CorrelaciÃ³n dÃ©bil

---

### 2. EstadÃ­sticas Descriptivas (descriptive_stats.parquet)

**Contenido:**
- 11 estadÃ­sticas por cada variable numÃ©rica
- MÃ©tricas: count, mean, std_dev, min, Q1, median, Q3, max, IQR, skewness, kurtosis

**VisualizaciÃ³n en dashboard:**
- Tabla estilizada con gradiente de color
- GrÃ¡fico de barras con error bars (media Â± desviaciÃ³n estÃ¡ndar)
- Box plots interactivos mostrando los 5 nÃºmeros resumen

**Ejemplo de uso:**
```powershell
# Cargar en dashboard
# 1. Seleccionar: descriptive_stats.parquet
# 2. Ir a Tab 7: "Exploratory Analysis (EDA)"
# 3. Ver tabla y grÃ¡ficos de distribuciÃ³n
```

**InterpretaciÃ³n:**
- **Skewness < -1**: DistribuciÃ³n sesgada izquierda
- **Skewness -0.5 a 0.5**: Aproximadamente simÃ©trica
- **Skewness > 1**: DistribuciÃ³n sesgada derecha
- **Kurtosis < 0**: PlaticÃºrtica (colas ligeras)
- **Kurtosis â‰ˆ 0**: MesocÃºrtica (normal)
- **Kurtosis > 0**: LeptocÃºrtica (colas pesadas)

---

### 3. Pruebas Chi-Cuadrado (chi_square_tests.parquet)

**Contenido:**
- Pruebas de independencia entre variables categÃ³ricas
- Tests: Continente vs Temperatura, EstaciÃ³n vs Temperatura, PerÃ­odo vs Temperatura
- Incluye: estadÃ­stico Ï‡Â², p-value, grados de libertad, significancia

**VisualizaciÃ³n en dashboard:**
- Tabla de resultados con resaltado de tests significativos
- GrÃ¡fico de barras del estadÃ­stico Ï‡Â²
- InterpretaciÃ³n textual automÃ¡tica

**Ejemplo de uso:**
```powershell
# Cargar en dashboard
# 1. Seleccionar: chi_square_tests.parquet
# 2. Ir a Tab 7: "Exploratory Analysis (EDA)"
# 3. Ver resultados de pruebas de independencia
```

**InterpretaciÃ³n:**
- **p-value < 0.05**: Variables dependientes (relaciÃ³n significativa) âœ…
- **p-value â‰¥ 0.05**: Variables independientes (no hay relaciÃ³n) âŒ

**Ejemplo de resultado:**
```
Test: Continent vs Temperature Category
Ï‡Â² = 145,678.23
p-value = 0.0000 (< 0.05)
â†’ Significativo: La temperatura depende del continente
```

---

### Casos de Uso del EDA

**1. AnÃ¡lisis Pre-Modelado:**
- Detectar multicolinealidad antes de entrenar modelos ML
- Identificar variables dependientes
- Verificar distribuciones para decidir transformaciones

**2. ValidaciÃ³n de HipÃ³tesis:**
- Â¿Las temperaturas estÃ¡n aumentando? â†’ Ver correlaciÃ³n `year` â†” `avg_temperature`
- Â¿Hay diferencias por continente? â†’ Ver chi-square continente vs temperatura
- Â¿Las estaciones afectan? â†’ Ver chi-square estaciÃ³n vs temperatura

**3. DetecciÃ³n de AnomalÃ­as:**
- Usar IQR para detectar outliers: valores fuera de [Q1 - 1.5Ã—IQR, Q3 + 1.5Ã—IQR]
- Verificar skewness para distribuciones asimÃ©tricas
- Analizar kurtosis para eventos extremos

**4. Reportes CientÃ­ficos:**
- Exportar tablas de estadÃ­sticas descriptivas
- Incluir heatmaps de correlaciÃ³n
- Reportar resultados de tests estadÃ­sticos

---

## Script Completo de Procesamiento

Para procesar el dataset completo y generar todos los 11 archivos Parquet automÃ¡ticamente:

```powershell
# Desde el directorio raÃ­z del proyecto
.\scripts\process_full_dataset.ps1
```

Este script ejecuta todo el pipeline:
1. âœ… Verifica Docker
2. âœ… Inicia HDFS (namenode + datanode)
3. âœ… Sube el dataset completo a HDFS
4. âœ… Procesa con PySpark generando 11 archivos:
   - 8 agregaciones climÃ¡ticas
   - 3 anÃ¡lisis EDA
5. âœ… (Opcional) Descarga resultados a local

**ParÃ¡metros disponibles:**
```powershell
# Procesar todo y descargar
.\scripts\process_full_dataset.ps1

# Solo procesar (datos quedan en HDFS)
.\scripts\process_full_dataset.ps1 -SkipDownload

# Solo procesar (asume datos ya en HDFS)
.\scripts\process_full_dataset.ps1 -SkipUpload

# Solo procesar sin upload ni download
.\scripts\process_full_dataset.ps1 -SkipUpload -SkipDownload
```

**Tiempo estimado (dataset completo ~8.6M registros):**
- Upload a HDFS: ~2-5 minutos
- Procesamiento Spark: ~20-30 minutos
  - Agregaciones: ~15 minutos
  - EDA: ~10-15 minutos adicionales
- Download (opcional): ~2-3 minutos
- **Total: ~25-40 minutos**

**Archivos generados (11 total, ~150 MB):**
```
/data/climaxtreme/processed/
â”œâ”€â”€ monthly.parquet              (~80 MB)
â”œâ”€â”€ yearly.parquet               (~30 MB)
â”œâ”€â”€ anomalies.parquet            (~15 MB)
â”œâ”€â”€ climatology.parquet          (~10 MB)
â”œâ”€â”€ seasonal.parquet             (~8 MB)
â”œâ”€â”€ extreme_thresholds.parquet   (~5 MB)
â”œâ”€â”€ regional.parquet             (~1 MB)
â”œâ”€â”€ continental.parquet          (~500 KB)
â”œâ”€â”€ correlation_matrix.parquet   (<1 MB)
â”œâ”€â”€ descriptive_stats.parquet    (<1 MB)
â””â”€â”€ chi_square_tests.parquet     (<1 MB)
```
