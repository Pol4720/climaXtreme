# README_COPILOT.md

## Project Overview

This document explains what was generated by GitHub Copilot for the climaXtreme project - a comprehensive climate analysis platform built with Hadoop and PySpark.

## What Was Generated

### 1. Project Structure & Configuration

**Generated a complete Python package structure:**
- `src/climaxtreme/` - Main package with proper Python packaging
- `pyproject.toml` - Modern Python packaging configuration
- `requirements.txt` - Comprehensive dependency management
- `setup.py` equivalent through pyproject.toml
- `.gitignore` - Already present, but enhanced for Python/data science

**Development Environment Setup:**
- `configs/default_config.yml` - YAML-based configuration management
- `scripts/setup.sh` - Automated development environment setup
- `.pre-commit-config.yaml` - Code quality enforcement
- `Dockerfile` & `docker-compose.yml` - Containerized deployment

### 2. Core Data Processing Pipeline

**Data Ingestion Module (`src/climaxtreme/data/`)**
- `ingestion.py` - Berkeley Earth data download and management
  - Handles various Berkeley Earth data formats
  - Robust HTTP downloading with retries
  - Metadata tracking and file management
- `validation.py` - Comprehensive data quality checks
  - Missing value analysis
  - Outlier detection for climate data
  - Temporal consistency validation
  - Climate-specific range checks
  - **NEW:** Synthetic data validation with `SyntheticDataValidator` class

**PySpark Preprocessing (`src/climaxtreme/preprocessing/`)**
- `spark_processor.py` - Big data processing with PySpark
  - Optimized Spark session configuration
  - Climate data cleaning and aggregation
  - Anomaly detection using statistical methods
  - Parquet output for efficient storage
- `preprocessor.py` - Pandas-based processing for smaller datasets
  - Alternative to Spark for memory-fit data
  - Same functionality with traditional pandas
- **NEW:** `synthetic_generator.py` - Synthetic climate data generation
  - Physically consistent weather variable simulation
  - Climate zone-aware parameter adjustment
  - Storm tracking and alert generation
  - Extreme event simulation

### 3. Synthetic Data Generation (NEW)

**Synthetic Climate Generator (`src/climaxtreme/preprocessing/spark/synthetic_generator.py`)**

The synthetic data generator creates realistic climate data to extend the original dataset for advanced visualizations and ML model training:

**Features:**
- **Hourly Temperature Data** - Diurnal and seasonal cycles with climate zone adjustments
- **Weather Variables** - Rain (Gamma distribution), wind (Weibull), humidity, pressure
- **Extreme Events** - Heatwaves, cold snaps, droughts, hurricanes, tornadoes
- **Storm Tracking** - Realistic storm trajectories with Saffir-Simpson categories
- **Weather Alerts** - Multi-level alert system (green/yellow/orange/red)

**Statistical Models Used:**
- **Gamma Distribution** - Precipitation amounts
- **Weibull Distribution** - Wind speeds
- **Markov Chains** - Wet/dry state transitions
- **Sinusoidal Functions** - Diurnal and seasonal temperature cycles
- **Random Walk** - Storm trajectory simulation

**Usage:**
```bash
# Generate synthetic data
climaxtreme generate-synthetic --input-path DATA/GlobalLandTemperaturesByCity.csv --output-path DATA/synthetic

# With custom options
climaxtreme generate-synthetic \
  --input-path DATA/GlobalLandTemperaturesByCity.csv \
  --output-path DATA/synthetic \
  --seed 42 \
  --sample-fraction 0.1
```

**Generated Columns:**
| Column | Description | Distribution/Model |
|--------|-------------|-------------------|
| `temperature_hourly` | Hourly temperature (¬∞C) | Sinusoidal + noise |
| `rain_mm` | Precipitation (mm) | Gamma distribution |
| `wind_speed_kmh` | Wind speed (km/h) | Weibull distribution |
| `humidity_pct` | Relative humidity (%) | Normal, temp-correlated |
| `pressure_hpa` | Atmospheric pressure (hPa) | Normal distribution |
| `climate_zone` | Zone classification | Latitude-based |
| `event_type` | Extreme event type | Probabilistic |
| `event_intensity` | Event severity (0-10) | Event-specific |
| `storm_id` | Storm identifier | Generated |
| `storm_category` | Saffir-Simpson scale | Wind-speed based |
| `alert_level` | Alert severity | Threshold-based |
| `alert_type` | Alert category | Variable-based |

### 3. Advanced Analytics Modules

**Analysis Components (`src/climaxtreme/analysis/`)**
- `heatmap.py` - Temperature heatmap generation
  - Global temperature visualization
  - Seasonal pattern analysis
  - Temperature anomaly mapping
  - Multiple visualization backends (matplotlib, seaborn)
- `timeseries.py` - Time series analysis
  - Long-term trend analysis with statistical testing
  - Seasonal decomposition
  - Extreme event detection
  - Polynomial and linear trend fitting

### 4. Machine Learning Pipeline

**ML Models (`src/climaxtreme/ml/`)**
- `baseline.py` - Multiple regression models
  - Linear, Ridge, Lasso regression
  - Random Forest and Gradient Boosting
  - Time series cross-validation
  - Hyperparameter tuning with GridSearchCV
  - Feature engineering for climate data
- `predictor.py` - Ensemble methods
  - Voting regressor combining multiple models
  - Uncertainty quantification
  - Time series specific validation
  - Model persistence and loading

### 5. Interactive Dashboard

**Streamlit Application (`src/climaxtreme/dashboard/`)**
- `app.py` - Full-featured web dashboard
  - Data overview with quality metrics
  - Interactive temperature trend visualization
  - Dynamic heatmap generation
  - Seasonal analysis with box plots and climatology
  - Extreme event detection and visualization
  - Real-time data filtering and exploration
- `utils.py` - Dashboard utilities and data loading
  - HDFS and local file loading support
  - Synthetic data loading helpers
  - Demo data generation for visualization

**Dashboard Pages (NEW):**
- `1_üìà_Temporal_Analysis.py` - Long-term temperature trends
- `2_üå°Ô∏è_Anomalies.py` - Temperature anomaly detection
- `3_üçÇ_Seasonal_Analysis.py` - Seasonal patterns
- `4_‚ö°_Extreme_Events.py` - Extreme weather events
- `5__Country_Analysis.py` - Country-level analysis
- `6_üåê_Continental_Analysis.py` - Continental comparisons
- `7_üìä_Statistical_Analysis.py` - Statistical summaries
- `8_üó∫Ô∏è_Climate_Heatmaps.py` - **NEW:** Global temperature heatmaps
- `9_üåÄ_Storm_Tracking.py` - **NEW:** Storm trajectory visualization
- `10_üö®_Active_Alerts.py` - **NEW:** Weather alert dashboard
- `11_üîÆ_Intensity_Prediction.py` - **NEW:** ML intensity prediction
- `12_üìâ_Weather_TimeSeries.py` - **NEW:** Multi-variable time series
- `13_üìú_Historical_Comparison.py` - **NEW:** Historical trend comparison

### 6. CLI Interface

**Command Line Tool (`src/climaxtreme/cli.py`)**
- Complete CLI with Click framework
- Subcommands for all major operations:
  - `ingest` - Data download and ingestion
  - `preprocess` - PySpark data processing
  - `analyze` - Run analysis pipelines
  - `dashboard` - Launch web interface
  - `generate-synthetic` - **NEW:** Generate synthetic climate data
  - `train-intensity-model` - **NEW:** Train ML intensity prediction model
  - `stream-demo` - **NEW:** Run streaming simulation for real-time data testing

### 7. Streaming Module (NEW)

**Real-Time Streaming Simulation (`src/climaxtreme/streaming/`)**
- `streaming_demo.py` - Structured Streaming demo for real-time climate simulation
  - Markov chain-based weather state transitions
  - Multi-city parallel simulation
  - JSON output compatible with Spark Streaming
  - Weather alerts based on configurable thresholds

**Usage:**
```bash
# Run streaming demo for 60 seconds
climaxtreme stream-demo --duration 60 --n-cities 10

# With custom settings
climaxtreme stream-demo --duration 120 --output-path DATA/streaming --rate 20
```

### 8. Comprehensive Testing

**Test Suite (`tests/`)**
- `unit/` - Unit tests with pytest
  - `test_data_ingestion.py` - Mock-based ingestion testing
  - `test_baseline_model.py` - ML model validation
  - `test_config.py` - Configuration management tests
  - `test_synthetic_generator.py` - **NEW:** Synthetic generator validation
- `integration/` - End-to-end pipeline testing
  - `test_pipeline.py` - Full workflow validation
  - `test_synthetic_pipeline.py` - **NEW:** Synthetic data pipeline integration tests
  - Error handling and data consistency checks

### 9. DevOps & CI/CD

**GitHub Actions Workflow (`.github/workflows/ci.yml`)**
- Multi-version Python testing (3.9, 3.10, 3.11)
- Code quality checks (black, flake8, mypy)
- Automated testing with coverage reporting
- Package building and validation
- Docker image building and publishing

### 9. Utility & Configuration

**Utils Package (`src/climaxtreme/utils/`)**
- `config.py` - Configuration management with YAML
- `logging_config.py` - Structured logging setup
- Environment-specific configuration loading

### 10. Documentation & Examples

**Project Documentation:**
- Comprehensive `README.md` with usage examples
- Docker deployment instructions
- Development setup guide
- API documentation in docstrings
- **NEW:** `documentation/SYNTHETIC_DATA_AND_MODELS.md` - Technical documentation for synthetic data generation models and techniques

## Key Features Implemented

### 1. Type Hints Throughout
- Full type annotations using Python typing module
- mypy configuration for static type checking
- Consistent type hints across all modules

### 2. Error Handling & Logging
- Comprehensive error handling with meaningful messages
- Structured logging configuration
- Graceful failure handling in data pipelines

### 3. Configuration Management
- YAML-based configuration with defaults
- Environment-specific overrides
- Validation and type safety for config values

### 4. Code Quality Standards
- Black formatting (88 character line length)
- Flake8 linting with climate data specific rules
- Pre-commit hooks for automatic code quality
- Comprehensive docstrings following Google style

### 5. Big Data Optimizations
- PySpark configuration tuning for climate data
- Efficient data formats (Parquet)
- Memory-conscious data processing
- Scalable architecture design

### 6. Climate Data Specific Features
- Berkeley Earth data format handling
- Climate-appropriate data validation ranges
- Seasonal analysis and climatology calculations
- Temperature anomaly detection algorithms

## Architecture Decisions

### 1. Modular Design
- Separated concerns into distinct modules
- Plugin-like architecture for analysis components
- Easy to extend with new data sources or analysis methods

### 2. Dual Processing Paths
- PySpark for big data processing
- Pandas fallback for smaller datasets
- Consistent API across both approaches

### 3. Modern Python Packaging
- Used pyproject.toml instead of setup.py
- Follows current Python packaging standards
- Easy installation and dependency management

### 4. Comprehensive Testing Strategy
- Unit tests with mocking for external dependencies
- Integration tests for end-to-end workflows
- Automated testing in CI/CD pipeline

### 5. Production-Ready Features
- Docker containerization
- Configuration management
- Logging and monitoring capabilities
- Health checks and error recovery

## What This Enables

The generated codebase provides:

1. **Complete Climate Analysis Platform** - From raw data ingestion to advanced analytics
2. **Scalable Processing** - Handles datasets from MB to TB scale
3. **Interactive Exploration** - Web-based dashboard for non-technical users
4. **Reproducible Research** - Version-controlled analysis with consistent environments
5. **Production Deployment** - Docker-based deployment with CI/CD
6. **Extensible Architecture** - Easy to add new data sources, analysis methods, or ML models

## Getting Started

The project is immediately usable after generation:

1. Run `./scripts/setup.sh` for automated setup
2. Use `climaxtreme ingest` to download data
3. Process with `climaxtreme preprocess`
4. Explore via `climaxtreme dashboard`

This creates a professional-grade climate analysis platform suitable for research, education, or production use.

## Dashboard Big Data mode (memory-safe)

The Streamlit dashboard now includes memory protection features designed for large datasets:

- Big data mode (enabled by default):
  - CSV is read in chunks up to a configurable "Max rows to load" (default: 1,000,000)
  - Parquet is read fully and then randomly sampled to the same limit
  - Numeric columns are downcast to reduce RAM
  - Date parsing adds year/month/day in-place without duplicating the DataFrame
- Max points to plot (default: 200,000):
  - Scatter/box plots are downsampled to this cap to avoid oversized WebSocket payloads and browser freezes
- Heatmap anomalies are computed with vectorized operations (no row-wise apply)

If you experienced the app freezing or Tornado/WebSocket exceptions, keep Big data mode on and tune the limits in the sidebar according to your machine's RAM.