# üìä Parquet Files Documentation

Este documento describe la estructura y el proceso de generaci√≥n de todos los archivos Parquet producidos por el sistema climaXtreme.

## üìÅ Resumen General

El sistema genera **11 archivos Parquet** que contienen diferentes niveles de agregaci√≥n y an√°lisis de datos clim√°ticos hist√≥ricos. Todos estos archivos se generan mediante **Apache Spark** en el contenedor `processor` y se almacenan en **HDFS** bajo el directorio `/data/processed/`.

### Lista de Archivos Generados

| Archivo | Descripci√≥n | Registros Aprox. | Funci√≥n Generadora |
|---------|-------------|------------------|-------------------|
| `monthly.parquet` | Agregaciones mensuales por ciudad | ~8.6M | `aggregate_monthly_data()` |
| `yearly.parquet` | Agregaciones anuales por ciudad | ~350K | `aggregate_yearly_data()` |
| `anomalies.parquet` | Desviaciones de temperatura respecto a la media climatol√≥gica | ~350K | `detect_anomalies()` |
| `climatology.parquet` | Valores climatol√≥gicos promedio por ciudad y mes | ~170K | `compute_climatology_stats()` |
| `seasonal.parquet` | Agregaciones por estaci√≥n del a√±o | ~1M | `compute_seasonal_stats()` |
| `extreme_thresholds.parquet` | Umbrales de temperaturas extremas (P10, P90) | ~170K | `compute_extreme_thresholds()` |
| `regional.parquet` | Agregaciones por regi√≥n geogr√°fica | ~2K | `compute_regional_aggregations()` |
| `continental.parquet` | Agregaciones por continente | ~300 | `compute_continental_aggregations()` |
| `correlation_matrix.parquet` | Matriz de correlaci√≥n de Pearson | ~25 | `compute_correlation_matrix()` |
| `descriptive_stats.parquet` | Estad√≠sticas descriptivas completas | ~4 | `compute_descriptive_statistics()` |
| `chi_square_tests.parquet` | Pruebas de independencia Chi-cuadrado | ~3 | `compute_chi_square_tests()` |

---

## üìã Descripci√≥n Detallada de Cada Parquet

### 1Ô∏è‚É£ `monthly.parquet`

**Prop√≥sito:** Proporciona agregaciones mensuales de temperatura para cada ciudad, incluyendo estad√≠sticas descriptivas completas.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del pa√≠s |
| `Latitude` | string | Latitud en formato original (ej: "57.05N") |
| `Longitude` | string | Longitud en formato original (ej: "10.33E") |
| `year` | integer | A√±o |
| `month` | integer | Mes (1-12) |
| `avg_temperature` | double | Temperatura promedio mensual (¬∞C) |
| `min_temperature` | double | Temperatura m√≠nima mensual (¬∞C) |
| `max_temperature` | double | Temperatura m√°xima mensual (¬∞C) |
| `std_temperature` | double | Desviaci√≥n est√°ndar de la temperatura |
| `record_count` | long | N√∫mero de registros diarios utilizados |

**Generaci√≥n:**
```python
# Funci√≥n: compute_monthly_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 309-353)

df_monthly = df.groupBy("City", "Country", "Latitude", "Longitude", "year", "month").agg(
    F.mean("AverageTemperature").alias("avg_temperature"),
    F.min("AverageTemperature").alias("min_temperature"),
    F.max("AverageTemperature").alias("max_temperature"),
    F.stddev("AverageTemperature").alias("std_temperature"),
    F.count("*").alias("record_count")
)
```

**Caso de Uso:** An√°lisis detallado de tendencias mensuales, series temporales, visualizaci√≥n de ciclos estacionales por ciudad.

---

### 2Ô∏è‚É£ `yearly.parquet`

**Prop√≥sito:** Agrega datos a nivel anual para facilitar an√°lisis de tendencias a largo plazo.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del pa√≠s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `year` | integer | A√±o |
| `avg_temperature` | double | Temperatura promedio anual (¬∞C) |
| `min_temperature` | double | Temperatura m√≠nima anual (¬∞C) |
| `max_temperature` | double | Temperatura m√°xima anual (¬∞C) |
| `temperature_range` | double | Rango de temperatura (max - min) |
| `record_count` | long | N√∫mero de registros mensuales utilizados |

**Generaci√≥n:**
```python
# Funci√≥n: compute_yearly_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 355-400)

df_yearly = df_monthly.groupBy("City", "Country", "Latitude", "Longitude", "year").agg(
    F.mean("avg_temperature").alias("avg_temperature"),
    F.min("min_temperature").alias("min_temperature"),
    F.max("max_temperature").alias("max_temperature"),
    (F.max("max_temperature") - F.min("min_temperature")).alias("temperature_range"),
    F.sum("record_count").alias("record_count")
)
```

**Caso de Uso:** Visualizaci√≥n de tendencias anuales, detecci√≥n de cambios clim√°ticos a largo plazo, comparaciones interanuales.

---

### 3Ô∏è‚É£ `anomalies.parquet`

**Prop√≥sito:** Calcula las desviaciones (anomal√≠as) de temperatura respecto a la media climatol√≥gica de referencia.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del pa√≠s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `year` | integer | A√±o |
| `avg_temperature` | double | Temperatura promedio anual observada (¬∞C) |
| `climatology` | double | Temperatura climatol√≥gica de referencia (¬∞C) |
| `anomaly` | double | Anomal√≠a (observada - climatolog√≠a) (¬∞C) |
| `record_count` | long | N√∫mero de registros utilizados |

**Generaci√≥n:**
```python
# Funci√≥n: compute_anomalies()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 442-488)

# Se calcula la climatolog√≠a (promedio de todos los a√±os)
df_climatology = df_yearly.groupBy("City", "Country", "Latitude", "Longitude").agg(
    F.mean("avg_temperature").alias("climatology")
)

# Se hace JOIN y se calcula la anomal√≠a
df_anomalies = df_yearly.join(df_climatology, on=["City", "Country", "Latitude", "Longitude"])
df_anomalies = df_anomalies.withColumn("anomaly", 
    F.col("avg_temperature") - F.col("climatology")
)
```

**Caso de Uso:** Identificaci√≥n de a√±os at√≠picamente c√°lidos o fr√≠os, an√°lisis de variabilidad clim√°tica, estudios de cambio clim√°tico.

---

### 4Ô∏è‚É£ `climatology.parquet`

**Prop√≥sito:** Proporciona valores climatol√≥gicos de referencia (promedios hist√≥ricos) por ciudad y mes.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del pa√≠s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `month` | integer | Mes (1-12) |
| `climatology` | double | Temperatura climatol√≥gica mensual (¬∞C) |
| `record_count` | long | N√∫mero de a√±os utilizados en el c√°lculo |

**Generaci√≥n:**
```python
# Funci√≥n: compute_climatology()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 402-440)

df_climatology = df_monthly.groupBy("City", "Country", "Latitude", "Longitude", "month").agg(
    F.mean("avg_temperature").alias("climatology"),
    F.count("*").alias("record_count")
)
```

**Caso de Uso:** L√≠nea base para c√°lculo de anomal√≠as, comparaci√≥n de condiciones actuales vs. hist√≥ricas, normalizaci√≥n de datos.

---

### 5Ô∏è‚É£ `seasonal.parquet`

**Prop√≥sito:** Agrega datos por estaciones del a√±o para analizar patrones estacionales.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del pa√≠s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `year` | integer | A√±o |
| `season` | string | Estaci√≥n: "Winter", "Spring", "Summer", "Fall" |
| `avg_temperature` | double | Temperatura promedio estacional (¬∞C) |
| `min_temperature` | double | Temperatura m√≠nima estacional (¬∞C) |
| `max_temperature` | double | Temperatura m√°xima estacional (¬∞C) |
| `record_count` | long | N√∫mero de registros mensuales utilizados |

**Generaci√≥n:**
```python
# Funci√≥n: compute_seasonal_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 490-526)

# Se asigna la estaci√≥n seg√∫n el mes (hemisferio norte)
df_seasonal = df_monthly.withColumn("season",
    F.when((F.col("month") >= 12) | (F.col("month") <= 2), "Winter")
     .when((F.col("month") >= 3) & (F.col("month") <= 5), "Spring")
     .when((F.col("month") >= 6) & (F.col("month") <= 8), "Summer")
     .otherwise("Fall")
)

# Se agregan por estaci√≥n
df_seasonal = df_seasonal.groupBy("City", "Country", "Latitude", "Longitude", 
                                   "year", "season").agg(...)
```

**Caso de Uso:** An√°lisis de variaciones estacionales, identificaci√≥n de cambios en patrones estacionales, estudios de fenolog√≠a.

---

### 6Ô∏è‚É£ `extreme_thresholds.parquet`

**Prop√≥sito:** Calcula umbrales estad√≠sticos (percentiles 10 y 90) para identificar eventos extremos de temperatura.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `City` | string | Nombre de la ciudad |
| `Country` | string | Nombre del pa√≠s |
| `Latitude` | string | Latitud en formato original |
| `Longitude` | string | Longitud en formato original |
| `month` | integer | Mes (1-12) |
| `p10_temperature` | double | Percentil 10 de temperatura (fr√≠o extremo) (¬∞C) |
| `p90_temperature` | double | Percentil 90 de temperatura (calor extremo) (¬∞C) |

**Generaci√≥n:**
```python
# Funci√≥n: compute_extreme_thresholds()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 672-733)

df_extreme = df_monthly.groupBy("City", "Country", "Latitude", "Longitude", "month").agg(
    F.expr("percentile(avg_temperature, 0.1)").alias("p10_temperature"),
    F.expr("percentile(avg_temperature, 0.9)").alias("p90_temperature")
)
```

**Caso de Uso:** Detecci√≥n de olas de calor o fr√≠o extremo, alertas tempranas, an√°lisis de eventos clim√°ticos extremos.

---

### 7Ô∏è‚É£ `regional.parquet`

**Prop√≥sito:** Agrega datos por **16 regiones geogr√°ficas** del mundo, clasificadas autom√°ticamente mediante latitud y longitud.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `region` | string | Nombre de la regi√≥n (ej: "Central Europe", "East Asia") |
| `continent` | string | Continente al que pertenece la regi√≥n |
| `year` | integer | A√±o |
| `avg_temperature` | double | Temperatura promedio regional (¬∞C) |
| `min_temperature` | double | Temperatura m√≠nima regional (¬∞C) |
| `max_temperature` | double | Temperatura m√°xima regional (¬∞C) |
| `record_count` | long | N√∫mero de ciudades utilizadas |

**Regiones Geogr√°ficas (16):**
1. **Europe:** Northern Europe, Central Europe, Southern Europe
2. **Asia:** Northern Asia, Central Asia, South Asia, East Asia
3. **Africa:** Northern Africa, Central Africa, Southern Africa
4. **North America:** Northern North America, Central North America, Caribbean & Central America
5. **South America:** Northern South America, Central South America, Southern South America
6. **Oceania:** Northern Oceania, Southern Oceania
7. **Antarctica:** Antarctica

**Generaci√≥n:**
```python
# Funci√≥n: compute_regional_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 621-670)

# Paso 1: Parsear coordenadas (N/S/E/W ‚Üí num√©rico)
df_with_coords = df_yearly.withColumn("lat_numeric", parse_coordinates_udf(F.col("Latitude"), F.lit("lat")))
df_with_coords = df_with_coords.withColumn("lon_numeric", parse_coordinates_udf(F.col("Longitude"), F.lit("lon")))

# Paso 2: Asignar continente y regi√≥n
df_with_coords = df_with_coords.withColumn("continent", assign_continent_udf(F.col("lat_numeric"), F.col("lon_numeric")))
df_with_coords = df_with_coords.withColumn("region", assign_region_udf(F.col("lat_numeric"), F.col("lon_numeric")))

# Paso 3: Agregar por regi√≥n y a√±o
df_regional = df_with_coords.groupBy("region", "continent", "year").agg(
    F.mean("avg_temperature").alias("avg_temperature"),
    F.min("min_temperature").alias("min_temperature"),
    F.max("max_temperature").alias("max_temperature"),
    F.sum("record_count").alias("record_count")
)
```

**L√≥gica de Clasificaci√≥n Regional:**
- Se parsean las coordenadas de formato "57.05N, 10.33E" a valores num√©ricos con signo
- Se aplica una funci√≥n que clasifica seg√∫n rangos de latitud/longitud
- Cada regi√≥n tiene l√≠mites geogr√°ficos espec√≠ficos definidos en `assign_region()` (l√≠neas 583-619)

**Caso de Uso:** Comparaci√≥n de tendencias clim√°ticas entre regiones geogr√°ficas, mapas de calor regionales, dashboard interactivo con selecci√≥n de regiones.

---

### 8Ô∏è‚É£ `continental.parquet`

**Prop√≥sito:** Agrega datos al nivel m√°s alto: **7 continentes**, facilitando comparaciones globales.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `continent` | string | Nombre del continente: "Europe", "Asia", "Africa", "North America", "South America", "Oceania", "Antarctica" |
| `year` | integer | A√±o |
| `avg_temperature` | double | Temperatura promedio continental (¬∞C) |
| `min_temperature` | double | Temperatura m√≠nima continental (¬∞C) |
| `max_temperature` | double | Temperatura m√°xima continental (¬∞C) |
| `record_count` | long | N√∫mero de ciudades utilizadas |

**Generaci√≥n:**
```python
# Funci√≥n: compute_continental_aggregations()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 672-733)

# Similar a regional, pero solo agrupa por continente (sin regi√≥n)
df_continental = df_with_coords.groupBy("continent", "year").agg(
    F.mean("avg_temperature").alias("avg_temperature"),
    F.min("min_temperature").alias("min_temperature"),
    F.max("max_temperature").alias("max_temperature"),
    F.sum("record_count").alias("record_count")
)
```

**L√≥gica de Clasificaci√≥n Continental:**
```python
# Funci√≥n: assign_continent() (l√≠neas 547-581)
# Clasifica seg√∫n rangos de latitud y longitud:
- Europe: lat 36-72, lon -10-40
- Asia: lat -10-81, lon 26-180
- Africa: lat -35-37, lon -18-52
- North America: lat 15-72, lon -169-(-52)
- South America: lat -56-13, lon -82-(-34)
- Oceania: lat -47-(-10), lon 110-180
- Antarctica: lat < -60
```

**Caso de Uso:** Comparaciones globales, visualizaci√≥n de tendencias continentales, mapas interactivos con burbujas por continente.

---

## üîÑ Proceso de Generaci√≥n Completo

### Flujo de Procesamiento

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. INGESTA (HDFS)                                               ‚îÇ
‚îÇ    - Archivo: GlobalLandTemperaturesByCity.csv (~500 MB)       ‚îÇ
‚îÇ    - Ubicaci√≥n: hdfs://namenode:9000/data/raw/                 ‚îÇ
‚îÇ    - Registros: ~8.6 millones                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. PROCESAMIENTO SPARK (Container: processor)                  ‚îÇ
‚îÇ    - Script: spark_processor.py                                ‚îÇ
‚îÇ    - Funci√≥n principal: process_path()                          ‚îÇ
‚îÇ    - Orden de ejecuci√≥n:                                        ‚îÇ
‚îÇ      a) aggregate_monthly_data()            ‚Üí monthly.parquet  ‚îÇ
‚îÇ      b) aggregate_yearly_data()             ‚Üí yearly.parquet   ‚îÇ
‚îÇ      c) compute_climatology_stats()         ‚Üí climatology.parq ‚îÇ
‚îÇ      d) detect_anomalies()                  ‚Üí anomalies.parq   ‚îÇ
‚îÇ      e) compute_seasonal_stats()            ‚Üí seasonal.parquet ‚îÇ
‚îÇ      f) compute_extreme_thresholds()        ‚Üí extreme_thresh.p ‚îÇ
‚îÇ      g) compute_regional_aggregations()     ‚Üí regional.parquet ‚îÇ
‚îÇ      h) compute_continental_aggregations()  ‚Üí continental.parq ‚îÇ
‚îÇ      i) compute_correlation_matrix()        ‚Üí correlation_mat. ‚îÇ
‚îÇ      j) compute_descriptive_statistics()    ‚Üí descriptive_st.  ‚îÇ
‚îÇ      k) compute_chi_square_tests()          ‚Üí chi_square_tests ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. ALMACENAMIENTO (HDFS)                                        ‚îÇ
‚îÇ    - Ubicaci√≥n: hdfs://namenode:9000/data/processed/           ‚îÇ
‚îÇ    - Formato: Parquet (columnar, comprimido)                   ‚îÇ
‚îÇ    - Tama√±o total estimado: ~100-150 MB (11 archivos)          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. VISUALIZACI√ìN (Dashboard Streamlit)                         ‚îÇ
‚îÇ    - Acceso directo desde HDFS v√≠a PyArrow                     ‚îÇ
‚îÇ    - 7 pesta√±as de an√°lisis:                                   ‚îÇ
‚îÇ      ‚Ä¢ Temperature Trends                                       ‚îÇ
‚îÇ      ‚Ä¢ Heatmaps                                                 ‚îÇ
‚îÇ      ‚Ä¢ Seasonal Analysis                                        ‚îÇ
‚îÇ      ‚Ä¢ Extreme Events                                           ‚îÇ
‚îÇ      ‚Ä¢ Regional Analysis (con mapa interactivo)                ‚îÇ
‚îÇ      ‚Ä¢ Continental Analysis (con mapa global)                  ‚îÇ
‚îÇ      ‚Ä¢ Exploratory Analysis (EDA) ‚Üê NUEVO                      ‚îÇ
‚îÇ      ‚Ä¢ Continental Analysis (con mapa global)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Script de Ejecuci√≥n

**Archivo:** `scripts/process_full_dataset.ps1`

**Comando:**
```powershell
.\scripts\process_full_dataset.ps1
```

**Pasos internos del script:**

1. **Upload a HDFS:**
   ```bash
   docker exec -it namenode hdfs dfs -mkdir -p /data/raw
   docker exec -it namenode hdfs dfs -put /data/GlobalLandTemperaturesByCity.csv /data/raw/
   ```

2. **Ejecuci√≥n Spark:**
   ```bash
   docker exec processor spark-submit \
     --master local[*] \
     /opt/climaxtreme/src/climaxtreme/preprocessing/spark_processor.py \
     --input-path hdfs://namenode:9000/data/raw/GlobalLandTemperaturesByCity.csv \
     --output-path hdfs://namenode:9000/data/processed
   ```

3. **Verificaci√≥n:**
   ```bash
   docker exec namenode hdfs dfs -ls /data/processed/
   ```

4. **Descarga (Opcional):**
   ```bash
   docker exec namenode hdfs dfs -get /data/processed/*.parquet /data/processed/
   ```

---

## üõ†Ô∏è Funciones de Procesamiento

### Referencia R√°pida

| Funci√≥n | L√≠neas | Dependencias | Output |
|---------|--------|--------------|--------|
| `parse_coordinates()` | 528-545 | - | UDF para parsear "57.05N" ‚Üí 57.05 |
| `assign_continent()` | 547-581 | parse_coordinates | UDF para asignar continente |
| `assign_region()` | 583-619 | parse_coordinates | UDF para asignar regi√≥n |
| `compute_monthly_aggregations()` | 309-353 | df (raw) | monthly.parquet |
| `compute_yearly_aggregations()` | 355-400 | monthly.parquet | yearly.parquet |
| `compute_climatology()` | 402-440 | monthly.parquet | climatology.parquet |
| `compute_anomalies()` | 442-488 | yearly + climatology | anomalies.parquet |
| `compute_seasonal_aggregations()` | 490-526 | monthly.parquet | seasonal.parquet |
| `compute_extreme_thresholds()` | 672-733 | monthly.parquet | extreme_thresholds.parquet |
| `compute_regional_aggregations()` | 621-670 | yearly.parquet | regional.parquet |
| `compute_continental_aggregations()` | 672-733 | yearly.parquet (con coords) | continental.parquet |
| `process_path()` | 735-903 | Todas las anteriores | Orquesta todo el proceso |

### Dependencias entre Archivos

```
raw CSV
   ‚îú‚îÄ‚Üí monthly.parquet (base)
   ‚îÇ      ‚îú‚îÄ‚Üí yearly.parquet
   ‚îÇ      ‚îÇ      ‚îú‚îÄ‚Üí anomalies.parquet (yearly + climatology)
   ‚îÇ      ‚îÇ      ‚îú‚îÄ‚Üí regional.parquet (yearly + coords)
   ‚îÇ      ‚îÇ      ‚îî‚îÄ‚Üí continental.parquet (yearly + coords)
   ‚îÇ      ‚îú‚îÄ‚Üí climatology.parquet
   ‚îÇ      ‚îú‚îÄ‚Üí seasonal.parquet
   ‚îÇ      ‚îî‚îÄ‚Üí extreme_thresholds.parquet
```

---

## üìà Dashboard Integration

Los archivos Parquet se visualizan en el dashboard Streamlit mediante acceso directo a HDFS:

**Clase:** `HDFSReader` (app.py, l√≠neas 50-135)

**Ejemplo de lectura:**
```python
# Modo HDFS
reader = HDFSReader(namenode_host="namenode", namenode_port=9000)
df = reader.read_parquet("hdfs://namenode:9000/data/processed/regional.parquet")

# Modo Local
df = pd.read_parquet("./DATA/processed/regional.parquet")
```

**Pesta√±as del Dashboard:**

1. **Monthly Analysis** ‚Üí `monthly.parquet`
2. **Yearly Analysis** ‚Üí `yearly.parquet`
3. **Anomalies Analysis** ‚Üí `anomalies.parquet`
4. **Seasonal Analysis** ‚Üí `seasonal.parquet`
5. **Regional Analysis** ‚Üí `regional.parquet` + **Mapa interactivo del mundo**
6. **Continental Analysis** ‚Üí `continental.parquet` + **Mapa global con burbujas**

### Visualizaciones de Mapas (Nuevas)

**Regional Analysis Tab:**
- **Mapa Scatter Geo** con 16 regiones
- Burbujas proporcionales a temperatura
- Escala de colores: RdYlBu_r (azul=fr√≠o, rojo=caliente)
- Proyecci√≥n: Natural Earth
- Tooltip: Regi√≥n, Temperatura, # Registros

**Continental Analysis Tab:**
- **Mapa Scatter Geo** con 7 continentes
- Burbujas m√°s grandes con texto del continente
- Misma escala de colores
- Tooltip: Continente, Temperatura, # Registros

**Biblioteca Utilizada:** Plotly Express con soporte de `go.Scattergeo`

---

## ‚úÖ Validaci√≥n de Datos

### Checks de Calidad

Cada funci√≥n de procesamiento incluye:
- **Filtrado de valores nulos:** `.filter(F.col("column").isNotNull())`
- **Validaci√≥n de rango:** Temperaturas entre -90¬∞C y 60¬∞C
- **Conteo de registros:** Columna `record_count` para auditor√≠a

### Logs de Procesamiento

```python
logger.info(f"Monthly aggregations: {df_monthly.count()} records")
logger.info(f"Regional aggregations: {df_regional.count()} records")
logger.info(f"Continental aggregations: {df_continental.count()} records")
```

---

## üîß Configuraci√≥n

**Archivo:** `configs/default_config.yml`

```yaml
spark:
  app_name: "ClimaXtreme-Processor"
  master: "local[*]"
  input_path: "hdfs://namenode:9000/data/raw/GlobalLandTemperaturesByCity.csv"
  output_path: "hdfs://namenode:9000/data/processed"
```

---

### 9Ô∏è‚É£ `correlation_matrix.parquet`

**Prop√≥sito:** Matriz de correlaci√≥n de Pearson entre variables num√©ricas clim√°ticas para an√°lisis exploratorio.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `variable_1` | string | Nombre de la primera variable |
| `variable_2` | string | Nombre de la segunda variable |
| `correlation` | double | Coeficiente de correlaci√≥n de Pearson (-1 a 1) |
| `abs_correlation` | double | Valor absoluto de la correlaci√≥n |

**Generaci√≥n:**
```python
# Funci√≥n: compute_correlation_matrix()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 757-811)

# Variables analizadas:
# - year
# - avg_temperature
# - min_temperature
# - max_temperature
# - temperature_range (calculado como max - min)

# C√°lculo de correlaciones pairwise
for var1 in numeric_vars:
    for var2 in numeric_vars:
        correlation = df.stat.corr(var1, var2)
        # Se almacenan tanto la correlaci√≥n como su valor absoluto
```

**Interpretaci√≥n:**
- **r = 1**: Correlaci√≥n positiva perfecta
- **r = -1**: Correlaci√≥n negativa perfecta
- **r = 0**: Sin correlaci√≥n lineal
- **|r| > 0.7**: Correlaci√≥n fuerte
- **|r| > 0.4**: Correlaci√≥n moderada
- **|r| < 0.3**: Correlaci√≥n d√©bil

**Caso de Uso:** 
- Identificar relaciones lineales entre variables
- Detectar multicolinealidad antes de modelado
- Validar hip√≥tesis sobre dependencias clim√°ticas
- Visualizaci√≥n mediante heatmap de correlaciones

---

### üîü `descriptive_stats.parquet`

**Prop√≥sito:** Estad√≠sticas descriptivas completas para todas las variables num√©ricas del dataset.

**Esquema (Formato Pivotado):**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `variable` | string | Nombre de la variable analizada |
| `count` | double | N√∫mero de observaciones v√°lidas |
| `mean` | double | Media aritm√©tica |
| `std_dev` | double | Desviaci√≥n est√°ndar |
| `min` | double | Valor m√≠nimo |
| `q1` | double | Primer cuartil (percentil 25) |
| `median` | double | Mediana (percentil 50) |
| `q3` | double | Tercer cuartil (percentil 75) |
| `max` | double | Valor m√°ximo |
| `iqr` | double | Rango intercuart√≠lico (Q3 - Q1) |
| `skewness` | double | Asimetr√≠a de la distribuci√≥n |
| `kurtosis` | double | Curtosis (exceso de kurtosis) |

**Variables Analizadas:**
- `avg_temperature`
- `min_temperature`
- `max_temperature`
- `uncertainty` (incertidumbre de medici√≥n)

**Generaci√≥n:**
```python
# Funci√≥n: compute_descriptive_statistics()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 813-920)

# Para cada variable num√©rica:
stats = df.agg(
    count(var),
    mean(var),
    stddev(var),
    min(var),
    max(var),
    skewness(var),
    kurtosis(var),
    percentile_approx(var, 0.25),  # Q1
    percentile_approx(var, 0.50),  # Mediana
    percentile_approx(var, 0.75)   # Q3
)

iqr = Q3 - Q1  # Rango intercuart√≠lico
```

**Interpretaci√≥n:**

**Skewness (Asimetr√≠a):**
- **< -1**: Distribuci√≥n muy sesgada a la izquierda (cola larga izquierda)
- **-1 a -0.5**: Moderadamente sesgada izquierda
- **-0.5 a 0.5**: Aproximadamente sim√©trica
- **0.5 a 1**: Moderadamente sesgada derecha
- **> 1**: Muy sesgada a la derecha (cola larga derecha)

**Kurtosis (Curtosis):**
- **< 0**: Platic√∫rtica (colas m√°s ligeras que normal)
- **‚âà 0**: Mesoc√∫rtica (similar a distribuci√≥n normal)
- **> 0**: Leptoc√∫rtica (colas m√°s pesadas, picos m√°s pronunciados)

**Caso de Uso:**
- Comprender la distribuci√≥n de las variables
- Detectar outliers usando IQR
- Validar supuestos de normalidad
- Comparar dispersi√≥n entre variables
- Base para transformaciones de datos

---

### 1Ô∏è‚É£1Ô∏è‚É£ `chi_square_tests.parquet`

**Prop√≥sito:** Resultados de pruebas de independencia Chi-cuadrado para variables categ√≥ricas.

**Esquema:**

| Columna | Tipo | Descripci√≥n |
|---------|------|-------------|
| `test` | string | Nombre descriptivo del test |
| `variable_1` | string | Primera variable categ√≥rica |
| `variable_2` | string | Segunda variable categ√≥rica |
| `chi_square_statistic` | double | Estad√≠stico œá¬≤ calculado |
| `p_value` | double | Valor p del test |
| `degrees_of_freedom` | integer | Grados de libertad |
| `is_significant` | boolean | True si p < 0.05 (rechaza H‚ÇÄ) |

**Tests Realizados:**

1. **Continent vs Temperature Category**
   - Variables: `continent` (7 categor√≠as) √ó `temp_category` (Cold/Moderate/Hot)
   - Hip√≥tesis: ¬øLa distribuci√≥n de temperaturas es independiente del continente?

2. **Season vs Temperature Category**
   - Variables: `season` (4 estaciones) √ó `temp_category` (Cold/Moderate/Hot)
   - Hip√≥tesis: ¬øLas temperaturas var√≠an significativamente por estaci√≥n?

3. **Time Period vs Temperature Category**
   - Variables: `time_period` (Early/Late) √ó `temp_category` (Cold/Moderate/Hot)
   - Hip√≥tesis: ¬øHa cambiado la distribuci√≥n de temperaturas a lo largo del tiempo?

**Generaci√≥n:**
```python
# Funci√≥n: compute_chi_square_tests()
# Archivo: Tools/src/climaxtreme/preprocessing/spark_processor.py (l√≠neas 922-1044)

# 1. Crear categor√≠as de temperatura
df_categorized = df.withColumn('temp_category',
    when(col('avg_temperature') < 10, 'Cold')
    .when(col('avg_temperature') < 20, 'Moderate')
    .otherwise('Hot')
)

# 2. Crear tabla de contingencia
contingency = df.groupBy('var1', 'var2').count()

# 3. Calcular estad√≠stico Chi-cuadrado
chi_square = Œ£ ((Observed - Expected)¬≤ / Expected)

# 4. Calcular p-value usando distribuci√≥n œá¬≤
from scipy.stats import chi2
p_value = 1 - chi2.cdf(chi_square_stat, df)
```

**Interpretaci√≥n:**

**Hip√≥tesis:**
- **H‚ÇÄ (Nula)**: Las variables son independientes (no hay relaci√≥n)
- **H‚ÇÅ (Alternativa)**: Las variables son dependientes (existe relaci√≥n)

**Criterio de Decisi√≥n:**
- **p-value < 0.05**: Rechazar H‚ÇÄ ‚Üí Variables dependientes (**Significativo**)
- **p-value ‚â• 0.05**: No rechazar H‚ÇÄ ‚Üí Variables independientes (No significativo)

**Estad√≠stico œá¬≤:**
- Valores m√°s altos indican mayor discrepancia entre frecuencias observadas y esperadas
- Depende de los grados de libertad: `df = (filas - 1) √ó (columnas - 1)`

**Caso de Uso:**
- Validar hip√≥tesis sobre relaciones categ√≥ricas
- Detectar dependencias entre variables clim√°ticas y geogr√°ficas
- An√°lisis de varianza categ√≥rica
- Preparaci√≥n para modelos de clasificaci√≥n

---

## üìö Referencias

- **C√≥digo fuente:** `Tools/src/climaxtreme/preprocessing/spark_processor.py`
- **Dashboard:** `Tools/src/climaxtreme/dashboard/app.py`
- **Script de procesamiento:** `scripts/process_full_dataset.ps1`
- **Configuraci√≥n:** `Tools/configs/default_config.yml`

---

## üöÄ Pr√≥ximos Pasos

- [ ] Agregar tests unitarios para funciones de clasificaci√≥n geogr√°fica
- [ ] Implementar particionamiento de Parquet por a√±o para optimizar lecturas
- [ ] Crear √≠ndices espaciales para queries geogr√°ficas m√°s r√°pidas
- [ ] A√±adir soporte para datos en tiempo real (streaming)

---

**√öltima actualizaci√≥n:** 2024
**Autor:** Equipo climaXtreme
