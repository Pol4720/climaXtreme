{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# climaXtreme Getting Started\n",
    "\n",
    "This notebook demonstrates the basic usage of climaXtreme for climate data analysis.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Make sure you have installed climaXtreme and its dependencies:\n",
    "```bash\n",
    "pip install -e .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from climaxtreme.data import DataIngestion, DataValidator\n",
    "from climaxtreme.preprocessing import DataPreprocessor\n",
    "from climaxtreme.analysis import HeatmapAnalyzer, TimeSeriesAnalyzer\n",
    "from climaxtreme.ml import BaselineModel\n",
    "from climaxtreme.utils import setup_logging\n",
    "\n",
    "# Setup logging\n",
    "setup_logging(level=\"INFO\")\n",
    "\n",
    "print(\"climaXtreme modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion\n",
    "\n",
    "First, let's download some sample climate data from Berkeley Earth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data ingestion\n",
    "ingestion = DataIngestion(\"../data/raw\")\n",
    "\n",
    "# Download Berkeley Earth data for a small time range\n",
    "print(\"Downloading climate data...\")\n",
    "downloaded_files = ingestion.download_berkeley_earth_data(2020, 2022)\n",
    "\n",
    "print(f\"Downloaded {len(downloaded_files)} files:\")\n",
    "for file in downloaded_files:\n",
    "    info = ingestion.get_file_info(file)\n",
    "    if info[\"exists\"]:\n",
    "        print(f\"  - {file}: {info['size_mb']} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Validation\n",
    "\n",
    "Let's validate the downloaded data to check its quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize validator\n",
    "validator = DataValidator()\n",
    "\n",
    "# Validate data files\n",
    "validation_results = validator.validate_directory(Path(\"../data/raw\"))\n",
    "\n",
    "# Generate summary\n",
    "summary = validator.generate_validation_summary()\n",
    "print(f\"Validation Summary:\")\n",
    "print(f\"  Files validated: {summary['summary']['total_files_validated']}\")\n",
    "print(f\"  Success rate: {summary['summary']['success_rate']}%\")\n",
    "print(f\"  Total data rows: {summary['summary']['total_data_rows']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "Now let's preprocess the raw data to make it ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "# Process the main data file\n",
    "raw_files = list(Path(\"../data/raw\").glob(\"*.txt\"))\n",
    "if raw_files:\n",
    "    main_file = raw_files[0]  # Process the first file\n",
    "    print(f\"Processing {main_file.name}...\")\n",
    "    \n",
    "    output_files = preprocessor.process_file(str(main_file), \"../data/processed\")\n",
    "    \n",
    "    print(\"Processed files created:\")\n",
    "    for key, path in output_files.items():\n",
    "        if Path(path).exists():\n",
    "            print(f\"  - {key}: {path}\")\n",
    "else:\n",
    "    print(\"No raw data files found. Please run data ingestion first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Analysis\n",
    "\n",
    "Let's load the processed data and perform some basic analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed monthly data\n",
    "monthly_files = list(Path(\"../data/processed\").glob(\"*_monthly.csv\"))\n",
    "if monthly_files:\n",
    "    df = pd.read_csv(monthly_files[0])\n",
    "    print(f\"Loaded data shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Date range: {df['year'].min()}-{df['year'].max()}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    display(df.head())\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"\\nTemperature statistics:\")\n",
    "    display(df['avg_temperature'].describe())\n",
    "else:\n",
    "    print(\"No processed monthly data found. Please run preprocessing first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization\n",
    "\n",
    "Create some basic visualizations of the climate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and not df.empty:\n",
    "    # Time series plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df['year'] + df['month']/12, df['avg_temperature'], 'b-', alpha=0.7)\n",
    "    plt.title('Temperature Time Series')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Seasonal pattern\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    monthly_avg = df.groupby('month')['avg_temperature'].mean()\n",
    "    plt.plot(monthly_avg.index, monthly_avg.values, 'ro-', linewidth=2, markersize=8)\n",
    "    plt.title('Average Temperature by Month')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Temperature (°C)')\n",
    "    plt.xticks(range(1, 13), ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No data available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Analysis\n",
    "\n",
    "Use the built-in analysis modules for more sophisticated analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series analysis\n",
    "if 'df' in locals() and not df.empty:\n",
    "    ts_analyzer = TimeSeriesAnalyzer()\n",
    "    \n",
    "    try:\n",
    "        # Analyze temperature trends\n",
    "        trend_results = ts_analyzer.analyze_temperature_trends(\n",
    "            \"../data/processed\", \n",
    "            \"../data/output\"\n",
    "        )\n",
    "        \n",
    "        print(\"Temperature Trend Analysis:\")\n",
    "        linear_trend = trend_results['linear_trend']\n",
    "        print(f\"  Trend: {linear_trend['slope_per_decade']:.4f}°C per decade\")\n",
    "        print(f\"  R²: {linear_trend['r_squared']:.4f}\")\n",
    "        print(f\"  p-value: {linear_trend['p_value']:.6f}\")\n",
    "        print(f\"  Significant: {linear_trend['significant']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Trend analysis failed: {e}\")\n",
    "        print(\"This might be due to insufficient data for trend analysis\")\n",
    "else:\n",
    "    print(\"No data available for trend analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Machine Learning\n",
    "\n",
    "Train a simple model to predict temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df' in locals() and not df.empty and len(df) > 24:  # Need enough data for ML\n",
    "    # Initialize and train a baseline model\n",
    "    model = BaselineModel(\"random_forest\")\n",
    "    \n",
    "    # Add temperature column for model training\n",
    "    df_ml = df.rename(columns={'avg_temperature': 'temperature'})\n",
    "    \n",
    "    print(\"Training machine learning model...\")\n",
    "    results = model.train(df_ml)\n",
    "    \n",
    "    print(f\"Model Performance:\")\n",
    "    print(f\"  Training R²: {results['train_r2']:.4f}\")\n",
    "    print(f\"  Test R²: {results['test_r2']:.4f}\")\n",
    "    print(f\"  Test RMSE: {results['test_rmse']:.4f}\")\n",
    "    \n",
    "    # Feature importance (if available)\n",
    "    if 'feature_importance' in results:\n",
    "        print(\"\\nTop 5 Most Important Features:\")\n",
    "        importance = results['feature_importance']\n",
    "        for i, (feature, score) in enumerate(list(importance.items())[:5]):\n",
    "            print(f\"  {i+1}. {feature}: {score:.4f}\")\n",
    "    \n",
    "    # Make future predictions\n",
    "    print(\"\\nMaking future predictions...\")\n",
    "    future_predictions = model.predict_future(2024, 2024)\n",
    "    \n",
    "    print(f\"Predicted temperatures for 2024:\")\n",
    "    for idx, row in future_predictions.head().iterrows():\n",
    "        print(f\"  {row['year']}-{row['month']:02d}: {row['predicted_temperature']:.2f}°C\")\n",
    "        \n",
    "else:\n",
    "    print(\"Insufficient data for machine learning (need >24 records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook showed the basic usage of climaXtreme. For more advanced features:\n",
    "\n",
    "1. **Use PySpark preprocessing** for larger datasets:\n",
    "   ```python\n",
    "   from climaxtreme.preprocessing import SparkPreprocessor\n",
    "   with SparkPreprocessor() as processor:\n",
    "       processor.process_directory(\"data/raw\", \"data/processed\")\n",
    "   ```\n",
    "\n",
    "2. **Launch the interactive dashboard**:\n",
    "   ```bash\n",
    "   climaxtreme dashboard\n",
    "   ```\n",
    "\n",
    "3. **Use ensemble ML models**:\n",
    "   ```python\n",
    "   from climaxtreme.ml import ClimatePredictor\n",
    "   predictor = ClimatePredictor([\"linear\", \"ridge\", \"random_forest\"])\n",
    "   predictor.train_ensemble(df)\n",
    "   ```\n",
    "\n",
    "4. **Generate publication-quality heatmaps**:\n",
    "   ```python\n",
    "   from climaxtreme.analysis import HeatmapAnalyzer\n",
    "   analyzer = HeatmapAnalyzer()\n",
    "   analyzer.generate_global_heatmap(\"data/processed\", \"data/output\")\n",
    "   ```\n",
    "\n",
    "Check the full documentation in the README.md for complete usage examples!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}