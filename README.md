# climaXtreme ğŸŒ¡ï¸

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Climate analysis and extreme event modeling using Hadoop and PySpark for large-scale climate data processing. This project provides a comprehensive toolkit for ingesting, processing, analyzing, and visualizing Berkeley Earth climate data.

## ğŸŒŸ Features

- **Data Ingestion**: Automated download and ingestion of Berkeley Earth climate datasets
- **Big Data Processing**: PySpark-based preprocessing for large-scale climate data
- **Advanced Analytics**: Temperature trend analysis, seasonal patterns, and extreme event detection
- **Interactive Dashboard**: Streamlit-based web interface for data exploration and visualization
- **Machine Learning**: Baseline models for climate prediction with ensemble methods
- **Comprehensive Testing**: Full test coverage with unit and integration tests
- **CI/CD Pipeline**: Automated testing, linting, and deployment workflows

## ğŸš€ Quick Start

### Prerequisites

- Python 3.9 or higher
- Java 11 (required for PySpark)
- Git

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/Pol4720/climaXtreme.git
   cd climaXtreme
   ```

2. **Run the setup script**
   ```bash
   chmod +x scripts/setup.sh
   ./scripts/setup.sh
   ```

3. **Activate the virtual environment**
   ```bash
   source venv/bin/activate
   ```

### Manual Installation

If you prefer manual installation:

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -e .

# Create directory structure
mkdir -p data/{raw,processed,output} logs models
```

## ğŸ“Š Usage

### Command Line Interface

climaXtreme provides a comprehensive CLI for all operations:

```bash
# Download climate data
climaxtreme ingest --start-year 2020 --end-year 2023

# Preprocess data using PySpark (local files)
climaxtreme preprocess --input-dir data/raw --output-dir data/processed

# Preprocess data directly from HDFS (CSV or Berkeley Earth .txt)
# Example with the sample CSV uploaded to HDFS (see HDFS section below)
climaxtreme preprocess \
   --input-path hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv \
   --output-path hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed \
   --format city-csv

# Run analysis
climaxtreme analyze --data-path data/processed --analysis-type both

# Launch interactive dashboard
climaxtreme dashboard --host localhost --port 8501
```

### Python API

```python
from climaxtreme import DataIngestion, SparkPreprocessor, HeatmapAnalyzer

# Data ingestion
ingestion = DataIngestion("data/raw")
files = ingestion.download_berkeley_earth_data(2020, 2023)

# Preprocessing with PySpark
with SparkPreprocessor() as preprocessor:
    results = preprocessor.process_directory("data/raw", "data/processed")

# Generate heatmaps
analyzer = HeatmapAnalyzer()
heatmap_path = analyzer.generate_global_heatmap("data/processed", "data/output")
```

### HDFS (Local) with Docker on Windows

The project supports reading/writing directly to HDFS. For local development on Windows, a lightweight HDFS cluster is provided via Docker.

**ğŸ“– Detailed setup guide**: See [HDFS_SETUP_GUIDE.md](HDFS_SETUP_GUIDE.md) for complete instructions and troubleshooting.

**Quick start:**

1) **Ensure Docker Desktop is running** (critical!)

2) Start HDFS and upload a sample of the dataset (PowerShell):

```powershell
# From repo root, in PowerShell
.\scripts\hdfs_setup_and_load.ps1 -CsvPath "DATA\GlobalLandTemperaturesByCity.csv" -Head 100000
```

This will:
- Download Apache Hadoop images (first time: ~500MB, 2-3 minutes)
- Start NameNode/DataNode via `infra/docker-compose.yml`
- Create `/data/climaxtreme` in HDFS
- Upload `GlobalLandTemperaturesByCity_sample.csv` (first 100k lines)

3) Run preprocessing against HDFS (PowerShell example):

```powershell
climaxtreme preprocess `
   --input-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/GlobalLandTemperaturesByCity_sample.csv" `
   --output-path "hdfs://climaxtreme-namenode:9000/data/climaxtreme/processed" `
   --format city-csv
```

Outputs (Parquet) will be written under `/data/climaxtreme/processed` in HDFS (subfolders `monthly.parquet`, `yearly.parquet`, `anomalies.parquet`).

4) Launch the Streamlit dashboard:

```powershell
climaxtreme dashboard --data-dir "DATA"
```

Access at: http://localhost:8501

**Verification:**
- NameNode UI: http://localhost:9870 (browse HDFS files)
- List HDFS: `docker exec climaxtreme-namenode hdfs dfs -ls /data/climaxtreme`

**Stop HDFS:** `docker compose -f infra\docker-compose.yml down`

Prerequisites for this section:
- Docker Desktop installed and **running** (WSL2 backend recommended)
- Internet access to pull Hadoop images on first run

### Docker Deployment

```bash
# Build and run with Docker Compose
docker-compose up -d

# Access dashboard at http://localhost:8501
# Access Jupyter at http://localhost:8888
```

## ğŸ“ Project Structure

```
climaXtreme/
â”œâ”€â”€ src/climaxtreme/           # Main package
â”‚   â”œâ”€â”€ data/                  # Data ingestion and validation
â”‚   â”œâ”€â”€ preprocessing/         # PySpark data processing
â”‚   â”œâ”€â”€ analysis/             # Analysis modules (heatmaps, time series)
â”‚   â”œâ”€â”€ dashboard/            # Streamlit dashboard
â”‚   â”œâ”€â”€ ml/                   # Machine learning models
â”‚   â””â”€â”€ utils/                # Utilities and configuration
â”œâ”€â”€ tests/                    # Test suite
â”‚   â”œâ”€â”€ unit/                 # Unit tests
â”‚   â””â”€â”€ integration/          # Integration tests
â”œâ”€â”€ docs/                     # Documentation
â”œâ”€â”€ notebooks/                # Jupyter notebooks
â”œâ”€â”€ scripts/                  # Setup and utility scripts
â”œâ”€â”€ configs/                  # Configuration files
â”œâ”€â”€ data/                     # Data directories
â”‚   â”œâ”€â”€ raw/                  # Raw data
â”‚   â”œâ”€â”€ processed/            # Processed data
â”‚   â””â”€â”€ output/               # Analysis outputs
â”œâ”€â”€ .github/workflows/        # CI/CD workflows
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ pyproject.toml           # Package configuration
â”œâ”€â”€ infra/                   # Local HDFS (Docker Compose)
â”‚   â””â”€â”€ docker-compose.yml   # NameNode/DataNode
â””â”€â”€ scripts/                 # Utility scripts (incl. Windows PowerShell for HDFS)
```

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/climaxtreme --cov-report=html

# Run specific test categories
pytest tests/unit/           # Unit tests only
pytest tests/integration/    # Integration tests only
```

## ğŸ”§ Development

### Code Quality

The project uses several tools to maintain code quality:

```bash
# Format code
black src/ tests/

# Lint code
flake8 src/ tests/

# Type checking
mypy src/climaxtreme/

# Pre-commit hooks (run automatically on commit)
pre-commit install
```

### Configuration

Configuration is managed through YAML files in the `configs/` directory:

```yaml
# configs/default_config.yml
data_dir: "data"
spark_app_name: "climaXtreme"
dashboard_port: 8501
# ... more settings
```

## ğŸ“ˆ Analysis Capabilities

### Temperature Trend Analysis
- Long-term global temperature trends
- Statistical significance testing
- Seasonal decomposition
- Polynomial trend fitting

### Heatmap Generation
- Global temperature heatmaps
- Seasonal pattern visualization
- Temperature anomaly maps
- Regional comparison heatmaps

### Extreme Event Detection
- Percentile-based threshold detection
- Hot and cold extreme identification
- Temporal distribution analysis
- Frequency trend analysis

### Machine Learning
- Multiple baseline models (Linear, Ridge, Random Forest, Gradient Boosting)
- Ensemble methods with uncertainty quantification
- Time series cross-validation
- Feature importance analysis

## ğŸŒ Dashboard Features

The Streamlit dashboard provides:

- **Data Overview**: Dataset statistics and quality metrics
- **Temperature Trends**: Interactive trend visualization with statistical analysis
- **Heatmaps**: Dynamic temperature and anomaly heatmaps
- **Seasonal Analysis**: Monthly climatology and seasonal comparisons
- **Extreme Events**: Interactive extreme event detection and visualization

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Make your changes
4. Run tests (`pytest`)
5. Run code quality checks (`black`, `flake8`, `mypy`)
6. Commit your changes (`git commit -m 'Add amazing feature'`)
7. Push to the branch (`git push origin feature/amazing-feature`)
8. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Berkeley Earth** for providing high-quality climate data
- **Apache Spark** for big data processing capabilities
- **Streamlit** for the interactive dashboard framework
- **scikit-learn** for machine learning algorithms

## ğŸ“ Support

For questions, issues, or contributions:

- Open an issue on [GitHub Issues](https://github.com/Pol4720/climaXtreme/issues)
- Check the documentation in the `docs/` directory
- Review the example notebooks in `notebooks/`

---

**climaXtreme** - Empowering climate research through big data analytics ğŸŒ
