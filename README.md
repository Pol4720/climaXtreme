# climaXtreme ğŸŒ¡ï¸

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

Climate analysis and extreme event modeling using Hadoop and PySpark for large-scale climate data processing. This project provides a comprehensive toolkit for ingesting, processing, analyzing, and visualizing Berkeley Earth climate data with Big Data technologies.

## ğŸŒŸ Features

- **Big Data Processing**: PySpark-based preprocessing with HDFS for large-scale climate data (~8.6M records)
- **Advanced Analytics**: Temperature trend analysis, seasonal patterns, extreme event detection, and exploratory data analysis (EDA)
- **Interactive Dashboard**: Streamlit-based web interface running in Docker with real-time HDFS data access
- **Machine Learning**: Baseline models for climate prediction with ensemble methods
- **Comprehensive EDA**: Correlation matrices, descriptive statistics, and chi-square independence tests
- **Geographic Visualizations**: Interactive maps by region and continent with 16 regions and 7 continents
- **Production Ready**: Full Docker deployment with HDFS, Spark processor, and dashboard

## ğŸš€ Quick Start

### Prerequisites

- **Docker Desktop** installed and running (required)
- ~8GB RAM available
- ~2GB disk space for Docker volumes

### Setup and Execution

**ğŸ“– For complete setup and execution instructions, see:**

## **â¡ï¸ [HDFS_SETUP_GUIDE.md](HDFS_SETUP_GUIDE.md)** â¬…ï¸

The guide includes:
- âœ… Quick start (2 commands to run everything)
- âœ… Step-by-step instructions for Windows and Linux/macOS
- âœ… Full pipeline automation scripts
- âœ… Dashboard configuration
- âœ… Troubleshooting and common issues
- âœ… Development workflows

### Quick Command Reference

**Windows (PowerShell):**
```powershell
# 1. Start all containers
cd infra
docker-compose up -d

# 2. Process complete dataset
cd ..
.\scripts\windows\process_full_dataset.ps1 -SkipDownload

# 3. Open dashboard: http://localhost:8501
```

**Linux/macOS (Bash):**
```bash
# 1. Start all containers
cd infra
docker-compose up -d

# 2. Process complete dataset
cd ..
bash scripts/linux/process_full_dataset.sh --skip-download

# 3. Open dashboard: http://localhost:8501
```

## ğŸ“Š System Architecture

```
Docker Network (hdfs)
â”œâ”€â”€ namenode:9870 (HDFS Web UI)
â”œâ”€â”€ namenode:9000 (HDFS API)
â”œâ”€â”€ datanode (Storage)
â”œâ”€â”€ processor (PySpark)
â””â”€â”€ dashboard:8501 (Streamlit)
```

## ğŸ“ Generated Datasets

The processing pipeline generates 11 Parquet files (~150 MB total):

**Climate Aggregations (8 files):**
- `monthly.parquet` - Monthly temperature trends
- `yearly.parquet` - Yearly aggregations with trend lines
- `anomalies.parquet` - Temperature anomaly detection
- `climatology.parquet` - Long-term climatology
- `seasonal.parquet` - Seasonal patterns
- `extreme_thresholds.parquet` - Extreme event thresholds
- `regional.parquet` - Analysis by 16 geographic regions
- `continental.parquet` - Analysis by 7 continents

**Exploratory Data Analysis (3 files):**
- `correlation_matrix.parquet` - Pearson correlation matrix
- `descriptive_stats.parquet` - 11 statistical measures per variable
- `chi_square_tests.parquet` - Independence tests (continent, season, period vs temperature)

## ğŸ¨ Dashboard Features

- **Temperature Trends**: Interactive trend visualization with statistical analysis
- **Heatmaps**: Dynamic temperature and anomaly heatmaps
- **Seasonal Analysis**: Monthly climatology and seasonal comparisons
- **Extreme Events**: Interactive extreme event detection
- **Regional Analysis**: Analysis by region with interactive world map ğŸ—ºï¸
- **Continental Analysis**: Analysis by continent with global bubble map ğŸŒ
- **Exploratory Analysis (EDA)**: Correlations, descriptive stats, and chi-square tests ğŸ“Š

## ğŸ“ Project Structure

```
climaXtreme/
â”œâ”€â”€ Tools/src/climaxtreme/     # Main package
â”‚   â”œâ”€â”€ data/                   # Data ingestion and validation
â”‚   â”œâ”€â”€ preprocessing/          # PySpark data processing
â”‚   â”œâ”€â”€ analysis/               # Analysis modules (heatmaps, time series)
â”‚   â”œâ”€â”€ dashboard/              # Streamlit dashboard
â”‚   â”œâ”€â”€ ml/                     # Machine learning models
â”‚   â””â”€â”€ utils/                  # Utilities and configuration
â”œâ”€â”€ infra/                      # Docker infrastructure
â”‚   â”œâ”€â”€ docker-compose.yml      # HDFS + Processor + Dashboard
â”‚   â””â”€â”€ Dockerfile.processor    # Processor image
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ windows/                # PowerShell scripts for Windows
â”‚   â””â”€â”€ linux/                  # Bash scripts for Linux/macOS
â”œâ”€â”€ DATA/                       # Data directories
â”‚   â”œâ”€â”€ GlobalLandTemperaturesByCity.csv  # Raw dataset
â”‚   â””â”€â”€ processed/              # Processed Parquet files
â””â”€â”€ docs/                       # Additional documentation
    â”œâ”€â”€ HDFS_SETUP_GUIDE.md     # Complete setup guide (START HERE)
    â”œâ”€â”€ PARQUETS.md             # Parquet file schemas
    â”œâ”€â”€ EDA_IMPLEMENTATION.md   # EDA documentation
    â””â”€â”€ scripts/README.md       # Script documentation
```

## ğŸ“š Documentation

- **[HDFS_SETUP_GUIDE.md](HDFS_SETUP_GUIDE.md)** - Complete setup and execution guide (Windows & Linux)
- **[scripts/README.md](scripts/README.md)** - Detailed script documentation
- **[PARQUETS.md](PARQUETS.md)** - Parquet file structures and schemas
- **[EDA_IMPLEMENTATION.md](EDA_IMPLEMENTATION.md)** - Exploratory Data Analysis guide

## ğŸ”§ Development

For development workflows (modifying code, rebuilding containers, etc.), see the "Development" section in [HDFS_SETUP_GUIDE.md](HDFS_SETUP_GUIDE.md).

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/climaxtreme --cov-report=html
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Berkeley Earth** for providing high-quality climate data
- **Apache Spark** for big data processing capabilities
- **Apache Hadoop** for distributed storage (HDFS)
- **Streamlit** for the interactive dashboard framework

## ğŸ“ Support

For questions, issues, or contributions:

- Open an issue on [GitHub Issues](https://github.com/Pol4720/climaXtreme/issues)
- Check the [HDFS_SETUP_GUIDE.md](HDFS_SETUP_GUIDE.md) for setup help
- Review the [scripts/README.md](scripts/README.md) for script documentation

---

**climaXtreme** - Empowering climate research through big data analytics ğŸŒ
